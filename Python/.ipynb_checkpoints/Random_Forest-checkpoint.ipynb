{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Author: Elisa Warner\n",
    "Created: 4/16/2019\n",
    "\n",
    "Purpose: Check results of SVM against stock data\n",
    "\n",
    "Change Record:  \n",
    "6/11/2019 Made Time-Cross Validation be non-overlapping  \n",
    "6/13/2019 Changed Labels so they are based on de fact RUA Close, not SI. Also added Regression RF  \n",
    "6/14/2019 Changed Labels back to SI, and moving average, because better score  \n",
    "6/18/2019 Set the set shift to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Table Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "## Create confusion table\n",
    "###########################\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def prediction_box(predictions, ground_truth):\n",
    "    ## results\n",
    "    result = {'tp':0, 'fp':0, 'tn':0, 'fn':0}\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        #print(pred_test[i], y[i])\n",
    "        if predictions[i] == ground_truth[i] and ground_truth[i] == 1:\n",
    "            result['tp'] = result.get('tp', 0) + 1\n",
    "        elif predictions[i] == ground_truth[i] and ground_truth[i] == 0:\n",
    "            result['tn'] = result.get('tn', 0) + 1\n",
    "        elif predictions[i] != ground_truth[i] and ground_truth[i] == 1:\n",
    "            result['fn'] = result.get('fn', 0) + 1\n",
    "        else:\n",
    "            result['fp'] = result.get('fp', 0) + 1\n",
    "\n",
    "    m = np.array([[result['tp'], result['fn']],[result['fp'],result['fn']]])\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(m, cmap='Pastel1')\n",
    "    \n",
    "    for i in range(m.shape[0]):\n",
    "        for j in range(m.shape[1]):\n",
    "            plt.text(j, i, \"{:.2f}\".format(m[i,j]), ha=\"center\", va=\"center\")\n",
    "            plt.title('Predictions for Test Set')\n",
    "            ax.set_xticks([0, 1])\n",
    "            ax.set_yticks([0, 1])\n",
    "            ax.set_xticklabels([1, 0])\n",
    "            ax.set_yticklabels([1, 0])\n",
    "            plt.ylabel('Ground Truth')\n",
    "            plt.xlabel('Prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing(df):\n",
    "    for col in list(df):\n",
    "        colvals = df[col].values\n",
    "        new_colvals = []\n",
    "        for i in range(len(colvals)):\n",
    "            if colvals[i] == '.':\n",
    "                print('Missing found')\n",
    "                new_colvals.append(colvals[i-1])\n",
    "            elif pd.isnull(colvals[i]):\n",
    "                print('nan found')\n",
    "                new_colvals.append(colvals[i-1])\n",
    "            else:\n",
    "                new_colvals.append(colvals[i])\n",
    "        df[col] = new_colvals\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing_horz(df):\n",
    "    new_df = pd.DataFrame()\n",
    "    for i in range(df.shape[0]):\n",
    "        row = list(df.iloc[i].values)\n",
    "\n",
    "        if '.' in row:\n",
    "            idx_list = [i for i in range(len(row)) if row[i] == ('.')]\n",
    "            for idx in idx_list:\n",
    "                row[idx] = row[idx - 1]\n",
    "\n",
    "        idx_list = [i for i in range(len(row)) if pd.isnull(row[i]) == True]\n",
    "        for idx in idx_list:\n",
    "            row[idx] = row[idx - 1]\n",
    "            \n",
    "        new_df = new_df.append(pd.DataFrame(row).T, ignore_index = True)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -U scikit-learn scipy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = \"./Combined_Sets_from_Revised.csv\" #\"Draft_Google_Shorter.csv\" #Removed Missing\n",
    "\n",
    "train_pd = pd.read_csv(file1)\n",
    "print(train_pd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_pd[5902:7100] # Google Trends part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd = train_pd[1:7070] #[5902:7100]\n",
    "\n",
    "train = np.array(train_pd)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[:, :149] # drop google columns\n",
    "train = pd.DataFrame(train)\n",
    "train = train.dropna() # drop any nan rows\n",
    "train.shape\n",
    "train = np.array(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[:,2:] # drop date, keep label\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Time-Series Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_cross(array, time_shift = 0, size_train_set = 365, size_val_set = 100, set_shift = 0):\n",
    "    np.random.seed(100)\n",
    "    label_idx = 0\n",
    "\n",
    "    if time_shift == 0:\n",
    "        time_shift = size_train_set + size_val_set + set_shift\n",
    "        \n",
    "    train_size = train.shape\n",
    "    j = 0\n",
    "    group = 1\n",
    "    end = train_size[0]\n",
    "    trainDataPartition = [] # list\n",
    "    valDataPartition = []\n",
    "\n",
    "    #print(size_train_set, size_val_set, train_size[0])\n",
    "    while j + (size_train_set + size_val_set + set_shift) < train_size[0]:\n",
    "        trainset = []\n",
    "        valset = []\n",
    "\n",
    "        trainset = train[j:j+size_train_set, :] # array\n",
    "        valset = train[j+size_train_set+1+set_shift:j+size_train_set+size_val_set+set_shift, :]\n",
    "\n",
    "        trainDataPartition.append(trainset)\n",
    "        valDataPartition.append(valset)\n",
    "    \n",
    "        group = group+1;\n",
    "        j = j + time_shift\n",
    "\n",
    "\n",
    "    # make last set -- decide if you want to throw out or adjust this set\n",
    "    print('Last set:', j, j+size_train_set)\n",
    "    trainset = train[j:j+size_train_set, :]\n",
    "    valset = train[j+size_train_set+1+set_shift:end, :]\n",
    "    #print(trainset)\n",
    "    #print(valset)\n",
    "    \n",
    "    trainDataPartition.append(trainset)\n",
    "    valDataPartition.append(valset)\n",
    "\n",
    "    \n",
    "    ### Optional: stratify\n",
    "    revised_trainDataPartition = []\n",
    "    revised_valDataPartition = []\n",
    "    \n",
    "    for i in range(len(trainDataPartition)):\n",
    "        trainset = trainDataPartition[i]\n",
    "        valset = valDataPartition[i] # Do the same for validation set\n",
    "        \n",
    "        print(trainset.shape[0], valset.shape[0])\n",
    "        records = trainset[:,0] # record of labels for training set\n",
    "        records_val = valset[:, 0] # record of labels for validation set\n",
    "        #print(sum(records == 1), sum(records == 0))\n",
    "        #print(sum(records_val == 1), sum(records_val == 0))\n",
    "        \n",
    "        ############ FOR TRAINING SET ################\n",
    "        if sum(records == 1) == 0 or sum(records == 0) == 0:\n",
    "            revised_trainDataPartition.append(np.nan)\n",
    "        elif sum(records == 1) > sum(records == 0):\n",
    "            while sum(records == 1) >= 1.3 * sum(records == 0):\n",
    "                r = round(np.random.rand() * (trainset.shape[0]-1))\n",
    "                #print(r)\n",
    "                if records[r] == 1:\n",
    "                    trainset = np.delete(trainset, r, 0)\n",
    "                    records = np.delete(records, r)\n",
    "\n",
    "            revised_trainDataPartition.append(trainset)\n",
    "            \n",
    "        else:\n",
    "            while sum(records == 1) <= 1.3 * sum(records == 0):\n",
    "                r = round(np.random.rand() * (trainset.shape[0]-1))\n",
    "                if records[r] == 0:\n",
    "                    trainset = np.delete(trainset, r, 0)\n",
    "                    records = np.delete(records, r)\n",
    "\n",
    "            revised_trainDataPartition.append(trainset)\n",
    "        #print(sum(records == 1), sum(records == 0))\n",
    "        \n",
    "        ############## FOR VALIDATION SET ################\n",
    "        if sum(records_val == 1) == 0 or sum(records_val == 0) == 0:\n",
    "            revised_valDataPartition.append(np.nan)\n",
    "        elif sum(records_val == 1) > sum(records_val == 0):\n",
    "            while sum(records_val == 1) >= 1.3 * sum(records_val == 0):\n",
    "                r = round(np.random.rand() * (valset.shape[0]-1))\n",
    "                #print(r)\n",
    "                if records_val[r] == 1:\n",
    "                    valset = np.delete(valset, r, 0)\n",
    "                    records_val = np.delete(records_val, r)\n",
    "            revised_valDataPartition.append(valset)\n",
    "        else:\n",
    "            while sum(records_val == 1) <= 1.3 * sum(records_val == 0):\n",
    "                r = round(np.random.rand() * (valset.shape[0]-1))\n",
    "                if records_val[r] == 0:\n",
    "                    valset = np.delete(valset, r, 0)\n",
    "                    records_val = np.delete(records_val, r)\n",
    "\n",
    "            revised_valDataPartition.append(valset)\n",
    "\n",
    "    group = len(revised_trainDataPartition)\n",
    "    group_val = len(revised_valDataPartition)\n",
    "    print('Number of groups:', group, group_val)\n",
    "    \n",
    "    return revised_trainDataPartition, revised_valDataPartition, group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataPartition, valDataPartition, group = time_cross(train, 0, 410, 260, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.DataFrame(valDataPartition[1])\n",
    "t.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainDataPartition[18]\n",
    "t = pd.DataFrame(trainDataPartition[0])\n",
    "t.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest Implementation\n",
    "No normalization necessary for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "### Try Random Forest Classifier\n",
    "\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf(train_list, val_list, group):\n",
    "    splits = group\n",
    "    score = []\n",
    "\n",
    "    #kf = sklearn.model_selection.KFold(n_splits=splits, random_state = 10, shuffle = True)\n",
    "    #kf.get_n_splits(features)\n",
    "\n",
    "    data_size = trainDataPartition[0].shape\n",
    "\n",
    "    for idx in range(len(trainDataPartition)-1):\n",
    "            # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "            try:\n",
    "                X_train, y_train = trainDataPartition[idx][:,1:data_size[1]+1], trainDataPartition[idx][:,0]\n",
    "                y_train = y_train.astype('int')\n",
    "                #print(X_train)\n",
    "                X_test, y_test = valDataPartition[idx][:,1:data_size[1]+1], valDataPartition[idx][:,0]\n",
    "                y_test = y_test.astype('int')\n",
    "                print('train:', sum(y_train), len(y_train))\n",
    "                print('test:', sum(y_test), len(y_test))\n",
    "\n",
    "                # Fit the RF model\n",
    "                clf = RandomForestClassifier(n_estimators=50, max_depth=1500, random_state=0)\n",
    "                clf.fit(X_train, y_train)\n",
    "                \n",
    "                # print predicitions\n",
    "                pred = clf.predict(X_test)\n",
    "                #print(pred)\n",
    "\n",
    "            except:\n",
    "                print('Skipped due to NaN')\n",
    "                continue # nan\n",
    "                \n",
    "            # add up AUROCs            \n",
    "            try:\n",
    "                temp_score = sklearn.metrics.roc_auc_score(y_test, pred)\n",
    "                #temp_score = sklearn.metrics.accuracy_score(y_test,pred)\n",
    "                #score = score + sklearn.metrics.accuracy_score(y_test, pred)\n",
    "                #temp_score = sum([1 for i in range(len(pred)) if pred[i] == y_test[i]]) / len(pred)\n",
    "                score.append(temp_score)\n",
    "                print(temp_score)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "    # calculate average\n",
    "    score = np.mean(score)\n",
    "    print(\"Averaged Score is: %0.4f\" % score, splits)\n",
    "\n",
    "\n",
    "    # print(clf.feature_importances_)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('groups', group)\n",
    "rf(valDataPartition, trainDataPartition, group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test\n",
    "# time_shift, size_train_set, size_val_set\n",
    "\n",
    "time_shift = [0] # smaller time shift is better, smaller training set, smaller val set 0, 50, 100, 126, 252, 504\n",
    "# 504 252 50 0.61268998\n",
    "size_train_set = [100, 252, 410]\n",
    "size_val_set = [50, 100, 260] # predict a month ahead (shift labels)\n",
    "\n",
    "score_matrix = np.zeros((len(time_shift), len(size_train_set), len(size_val_set)))\n",
    "settings_matrix = np.zeros((len(size_train_set), len(size_val_set)), dtype = 'i,i')\n",
    "\n",
    "for i in range(len(time_shift)):\n",
    "    for j in range(len(size_train_set)):\n",
    "        for k in range(len(size_val_set)):\n",
    "            print(time_shift, size_train_set[j], size_val_set[k])\n",
    "            trainDataPartition, valDataPartition, group = time_cross(train, time_shift[i], size_train_set[j], size_val_set[k], 0)\n",
    "\n",
    "            #time_shift[i]\n",
    "            try:\n",
    "                score_matrix[i][j][k] = rf(valDataPartition, trainDataPartition, group)\n",
    "            except:\n",
    "                print('Cannot compute score')\n",
    "            settings_matrix[j][k][0] = size_train_set[j]\n",
    "            settings_matrix[j][k][1] = size_val_set[k]\n",
    "            \n",
    "print(np.argmax(score_matrix, axis = 1))\n",
    "print(score_matrix)\n",
    "print(settings_matrix)\n",
    "#0.61268998 Didn't work so well with Label 2\n",
    "# Perfomed well with predicting next day with Google Trends 0.8975, best was 0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.max(score_matrix, axis = 1)\n",
    "#print(np.argmax(t, axis = 1))\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look for Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataPartition, valDataPartition, group = time_cross(train, 0, 252, 260, 0) #10 252 100 Draft_Google_shorter 0.8795"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = group\n",
    "score_list = []\n",
    "indices = []\n",
    "pred_labels = []\n",
    "\n",
    "data_size = trainDataPartition[0].shape\n",
    "\n",
    "for idx in range(len(trainDataPartition)):\n",
    "    try:\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, y_train = trainDataPartition[idx][:,1:data_size[1]+1], trainDataPartition[idx][:,0]\n",
    "        y_train = y_train.astype('int')\n",
    "        #print(X_train)\n",
    "        X_test, y_test = valDataPartition[idx][:,1:data_size[1]+1], valDataPartition[idx][:,0]\n",
    "        y_test = y_test.astype('int')\n",
    "        print('train:', sum(y_train), len(y_train))\n",
    "        print('test:', sum(y_test), len(y_test))\n",
    "    \n",
    "        # Fit the RF model\n",
    "        clf = RandomForestClassifier(n_estimators=50, max_depth=1500, random_state=0) # previously 7\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # print predicitions\n",
    "        pred = clf.predict(X_test)\n",
    "        pred_labels.append(pred)\n",
    "        print(pred)\n",
    "        #print(pred)\n",
    "    except:\n",
    "        print('Skipped')\n",
    "        continue #np.nan\n",
    "        \n",
    "    # add up AUROCs\n",
    "            \n",
    "    try:\n",
    "        temp_score = sklearn.metrics.roc_auc_score(y_test, pred)\n",
    "        score_list.append(temp_score)\n",
    "        indices.append(idx)\n",
    "        print(temp_score)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# calculate average\n",
    "score = np.mean(score_list)\n",
    "print(\"Averaged Score is: %0.4f\" % score, splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the accuracy through time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(\"Standard Deviation:\", np.std(score_list))\n",
    "print(indices)\n",
    "plt.figure()\n",
    "plt.plot(indices, score_list)\n",
    "plt.title('Performance across Folds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which features are important during the time when the datasets are accurate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [x for x in range(len(score_list)) if score_list[x] > 0.5]\n",
    "importance = np.array([])\n",
    "count = 0\n",
    "\n",
    "for idx in range(0, len(trainDataPartition)): # for several months of 2012, May - end of November, 1180, 1281\n",
    "    # run random forest\n",
    "    try:\n",
    "\n",
    "        X_train, y_train = trainDataPartition[idx][:,1:data_size[1]+1], trainDataPartition[idx][:,0]\n",
    "        y_train = y_train.astype('int')\n",
    "\n",
    "        X_test, y_test = valDataPartition[idx][:,1:data_size[1]+1], valDataPartition[idx][:,0]\n",
    "        y_test = y_test.astype('int')\n",
    "    \n",
    "        # Fit the RF model\n",
    "        clf = RandomForestClassifier(n_estimators=50, max_depth=1500, random_state=0) # previously 7\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # print predicitions\n",
    "        pred = clf.predict(X_test)\n",
    "    \n",
    "        if not importance.any():\n",
    "            print('hi')\n",
    "            importance = clf.feature_importances_\n",
    "        else:\n",
    "            importance = importance + clf.feature_importances_\n",
    "\n",
    "        count+=1\n",
    "    except:\n",
    "        continue #np.nan\n",
    "\n",
    "    try:\n",
    "        temp_score = sklearn.metrics.roc_auc_score(y_test, pred)\n",
    "        score_list.append(temp_score)\n",
    "        indices.append(idx)\n",
    "        print(temp_score)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = importance / count\n",
    "feature_indices = np.argsort(importance)[::-1]\n",
    "\n",
    "# Plot the feature importances of the forest (ERROR HIDDEN, TOO BIG)\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_train.shape[1]), importance[feature_indices],\n",
    "       color=\"r\", align=\"center\") # X_train.shape[1]\n",
    "plt.xticks(range(X_train.shape[1]), feature_indices) #X_train.shape[1]\n",
    "plt.xlim([-1, 100]) #X_train.shape[1]]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Combined_Sets_from_Revised.csv'\n",
    "df = pd.read_csv(name)\n",
    "df = df.drop(['Date'], axis = 1)\n",
    "\n",
    "# Show all the list of features and their respective importances\n",
    "\n",
    "#listoffeatures = list(train_pd)[1:]\n",
    "listoffeatures = list(df)\n",
    "shortlist = []\n",
    "\n",
    "print('Top Features listed by Importance')\n",
    "\n",
    "for i in range(len(feature_indices)-1):\n",
    "    idx = feature_indices[i]\n",
    "    print(listoffeatures[idx], importance[idx])\n",
    "    if i <= 25:\n",
    "        shortlist.append(listoffeatures[idx])\n",
    "shortlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph of fetaure importances\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot the feature importances of the forest (ERROR HIDDEN, TOO BIG)\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(100), importances[indices[0:100]],\n",
    "       color=\"r\", yerr=std[indices[0:100]], align=\"center\") # X_train.shape[1]\n",
    "plt.xticks(range(100), indices[0:100]) #X_train.shape[1]\n",
    "plt.xlim([-1, 100]) #X_train.shape[1]]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(clf.feature_importances_))\n",
    "print(len(list(train_pd)[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '../Data/Combined_Sets_from_Revised_3.csv'\n",
    "df = pd.read_csv(name)\n",
    "df = df.drop(['Date'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all the list of features and their respective importances\n",
    "\n",
    "#listoffeatures = list(train_pd)[1:]\n",
    "listoffeatures = list(df)\n",
    "shortlist = []\n",
    "\n",
    "print('Top Features listed by Importance')\n",
    "\n",
    "for i in range(len(indices)-1):\n",
    "    idx = indices[i]\n",
    "    print(listoffeatures[idx], importances[idx])\n",
    "#    if i <= 50:\n",
    "#        shortlist.append(listoffeatures[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataPartition, valDataPartition, group = time_cross(train, 0, 252, 260, 0) #10 252 100 Draft_Google_shorter 0.8795"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(valDataPartition[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = group\n",
    "score_list = []\n",
    "good_sets = []\n",
    "gammas = []\n",
    "indices = []\n",
    "\n",
    "data_size = trainDataPartition[0].shape\n",
    "\n",
    "for idx in range(len(trainDataPartition)-1):\n",
    "    try:\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, y_train = trainDataPartition[idx][:,1:data_size[1]+1], trainDataPartition[idx][:,0]\n",
    "        y_train = y_train.astype('int')\n",
    "        #print(X_train)\n",
    "        X_test, y_test = valDataPartition[idx][:,1:data_size[1]+1], valDataPartition[idx][:,0]\n",
    "        y_test = y_test.astype('int')\n",
    "        print('train:', sum(y_train), len(y_train))\n",
    "        print('test:', sum(y_test), len(y_test))\n",
    "        #print(1 / (X_train.shape[1] * X_train.var()))\n",
    "        # Fit the RF model\n",
    "\n",
    "        #gamma =  (1 / (X_train.shape[1] * X_train.var()))\n",
    "        clf = sklearn.svm.SVC(C=.4, kernel='rbf', gamma='scale') # previously 7\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        \n",
    "        # print predicitions\n",
    "        pred = clf.predict(X_test)\n",
    "        #print(pred)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # add up AUROCs\n",
    "            \n",
    "    try:\n",
    "        temp_score = sklearn.metrics.roc_auc_score(y_test, pred)\n",
    "        score_list.append(temp_score)\n",
    "        indices.append(idx)\n",
    "        print(temp_score)\n",
    "        if temp_score > 0.7:\n",
    "            good_sets.append(idx)\n",
    "            gammas.append(1 / (X_train.shape[1] * X_train.var()))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# calculate average\n",
    "score = np.mean(score_list)\n",
    "print(\"Averaged Score is: %0.4f\" % score, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(\"Standard Deviation:\", np.std(score_list))\n",
    "print(indices)\n",
    "plt.figure()\n",
    "plt.plot(indices, score_list)\n",
    "plt.title('Performance across Folds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels2 = []\n",
    "for predlist in pred_labels:\n",
    "    for x in range(len(predlist)):\n",
    "        pred_labels2.append(predlist[x])\n",
    "pred_labels2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_time_cross(array, time_shift = 0, size_train_set = 365, size_val_set = 100, set_shift = 0):\n",
    "    np.random.seed(100)\n",
    "    label_idx = 0\n",
    "\n",
    "    if time_shift == 0:\n",
    "        time_shift = size_train_set + size_val_set + set_shift\n",
    "        \n",
    "    train_size = train.shape\n",
    "    j = 0\n",
    "    group = 1\n",
    "    end = train_size[0]\n",
    "    trainDataPartition = [] # list\n",
    "    valDataPartition = []\n",
    "\n",
    "    #print(size_train_set, size_val_set, train_size[0])\n",
    "    while j + (size_train_set + size_val_set + set_shift) < train_size[0]:\n",
    "        trainset = []\n",
    "        valset = []\n",
    "\n",
    "        trainset = train[j:j+size_train_set, :] # array\n",
    "        valset = train[j+size_train_set+1+set_shift:j+size_train_set+size_val_set+set_shift, :]\n",
    "\n",
    "        trainDataPartition.append(trainset)\n",
    "        valDataPartition.append(valset)\n",
    "    \n",
    "        group = group+1;\n",
    "        j = j + time_shift\n",
    "\n",
    "\n",
    "    # make last set -- decide if you want to throw out or adjust this set\n",
    "    print('Last set:', j, j+size_train_set)\n",
    "    trainset = train[j:j+size_train_set, :]\n",
    "    valset = train[j+size_train_set+1+set_shift:end, :]\n",
    "    #print(trainset)\n",
    "    #print(valset)\n",
    "    \n",
    "    trainDataPartition.append(trainset)\n",
    "    valDataPartition.append(valset)\n",
    "\n",
    "    group = len(trainDataPartition)\n",
    "    group_val = len(valDataPartition)\n",
    "    print('Number of groups:', group, group_val)\n",
    "    \n",
    "    return trainDataPartition, valDataPartition, group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array(train_pd)\n",
    "train = train[:, :149] # drop google columns\n",
    "#train = train[5902:, :]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(train)\n",
    "train = train.dropna() # drop any nan rows\n",
    "train = np.array(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train[:,1]\n",
    "train = train[:, 3:train.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.reshape((train.shape[0], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(train)  \n",
    "PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
    "  svd_solver='auto', tol=0.0, whiten=False)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = pca.components_\n",
    "train2.shape\n",
    "train = train2.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.concatenate((labels, train), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[trainDataPartition, valDataPartition, group] = simple_time_cross(train, 0, 410, 100, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataPartition[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random/SVM Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = group\n",
    "score_list = []\n",
    "good_sets = []\n",
    "gammas = []\n",
    "indices = []\n",
    "\n",
    "data_size = trainDataPartition[0].shape\n",
    "\n",
    "for idx in range(len(trainDataPartition)-1):\n",
    "    try:\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, y_train = trainDataPartition[idx][:,1:data_size[1]+1], trainDataPartition[idx][:,0]\n",
    "        #y_train = y_train.astype('int')\n",
    "        #print(X_train)\n",
    "        X_test, y_test = valDataPartition[idx][:,1:data_size[1]+1], valDataPartition[idx][:,0]\n",
    "        #y_test = y_test.astype('int')\n",
    "        #print(1 / (X_train.shape[1] * X_train.var()))\n",
    "        # Fit the RF model\n",
    "\n",
    "        #gamma =  (1 / (X_train.shape[1] * X_train.var()))\n",
    "        #clf = SVR(C=1, epsilon=0.0001, gamma='scale', \\\n",
    "        #          kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "        clf = RandomForestRegressor(max_depth = 20, n_estimators=1500, random_state=4, criterion = 'mse') # previously 7\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        \n",
    "        # print predicitions\n",
    "        pred = clf.predict(X_test)\n",
    "        #print(pred)\n",
    "\n",
    "        plt.plot(y_test, label = 'ground_truth')\n",
    "        plt.plot(pred, label = 'predicted')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        print('Mean Squared Error:', metrics.mean_squared_error(y_test, pred))\n",
    "        # add up AUROCs\n",
    "    except:\n",
    "        continue\n",
    "            \n",
    "    #try:\n",
    "    #SS_tot = sum((y_test - np.mean(y_test))**2)\n",
    "    #S_res = sum((y_test - pred) ** 2)\n",
    "    #temp_score = 1 - (SS_res / SS_tot)\n",
    "    temp_score = metrics.r2_score(y_test, pred)\n",
    "    score_list.append(temp_score)\n",
    "    indices.append(idx)\n",
    "    print(temp_score)\n",
    "    if temp_score > 0.7:\n",
    "        good_sets.append(idx)\n",
    "        gammas.append(1 / (X_train.shape[1] * X_train.var()))\n",
    "    #except:\n",
    "    #    pass\n",
    "\n",
    "\n",
    "# calculate average\n",
    "score = np.mean(score_list)\n",
    "print(\"Averaged Score is: %0.4f\" % score, splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot by itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_test, label = 'ground_truth')\n",
    "plt.plot(pred, label = 'predicted')\n",
    "plt.legend()\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[:,95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataPartition[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
