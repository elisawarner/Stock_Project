{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Author: Elisa Warner\n",
    "Created: 4/16/2019\n",
    "\n",
    "Purpose: Check results of SVM against stock data\n",
    "\n",
    "Change Record:\n",
    "6/11/2019 Made Time-Cross Validation be non-overlapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Table Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "## Create confusion table\n",
    "###########################\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def prediction_box(predictions, ground_truth):\n",
    "    ## results\n",
    "    result = {'tp':0, 'fp':0, 'tn':0, 'fn':0}\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        #print(pred_test[i], y[i])\n",
    "        if predictions[i] == ground_truth[i] and ground_truth[i] == 1:\n",
    "            result['tp'] = result.get('tp', 0) + 1\n",
    "        elif predictions[i] == ground_truth[i] and ground_truth[i] == 0:\n",
    "            result['tn'] = result.get('tn', 0) + 1\n",
    "        elif predictions[i] != ground_truth[i] and ground_truth[i] == 1:\n",
    "            result['fn'] = result.get('fn', 0) + 1\n",
    "        else:\n",
    "            result['fp'] = result.get('fp', 0) + 1\n",
    "\n",
    "    m = np.array([[result['tp'], result['fn']],[result['fp'],result['fn']]])\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(m, cmap='Pastel1')\n",
    "    \n",
    "    for i in range(m.shape[0]):\n",
    "        for j in range(m.shape[1]):\n",
    "            plt.text(j, i, \"{:.2f}\".format(m[i,j]), ha=\"center\", va=\"center\")\n",
    "            plt.title('Predictions for Test Set')\n",
    "            ax.set_xticks([0, 1])\n",
    "            ax.set_yticks([0, 1])\n",
    "            ax.set_xticklabels([1, 0])\n",
    "            ax.set_yticklabels([1, 0])\n",
    "            plt.ylabel('Ground Truth')\n",
    "            plt.xlabel('Prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing(df):\n",
    "    for col in list(df):\n",
    "        colvals = df[col].values\n",
    "        new_colvals = []\n",
    "        for i in range(len(colvals)):\n",
    "            if colvals[i] == '.':\n",
    "                print('Missing found')\n",
    "                new_colvals.append(colvals[i-1])\n",
    "            elif pd.isnull(colvals[i]):\n",
    "                print('nan found')\n",
    "                new_colvals.append(colvals[i-1])\n",
    "            else:\n",
    "                new_colvals.append(colvals[i])\n",
    "        df[col] = new_colvals\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing_horz(df):\n",
    "    new_df = pd.DataFrame()\n",
    "    for i in range(df.shape[0]):\n",
    "        row = list(df.iloc[i].values)\n",
    "\n",
    "        if '.' in row:\n",
    "            idx_list = [i for i in range(len(row)) if row[i] == ('.')]\n",
    "            for idx in idx_list:\n",
    "                row[idx] = row[idx - 1]\n",
    "\n",
    "        idx_list = [i for i in range(len(row)) if pd.isnull(row[i]) == True]\n",
    "        for idx in idx_list:\n",
    "            row[idx] = row[idx - 1]\n",
    "            \n",
    "        new_df = new_df.append(pd.DataFrame(row).T, ignore_index = True)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -U scikit-learn scipy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7120, 422)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1 = \"./Combined_Sets_from_Revised.csv\" #\"Draft_Google_Shorter.csv\" #Removed Missing\n",
    "\n",
    "train_pd = pd.read_csv(file1)\n",
    "train_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_pd[5902:7100] # Google Trends part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7120, 422)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pd = train_pd #[5902:7100]\n",
    "\n",
    "train = np.array(train_pd)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[:, :148] # drop google columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 173.199997, 8.13, ..., 390.4, 395.35, 398.0],\n",
       "       [1, 172.779999, 8.16, ..., 395.35, 398.0, 403.7],\n",
       "       [1, 171.169998, 8.25, ..., 398.0, 403.7, 383.5],\n",
       "       ...,\n",
       "       [4, 1697.430054, 2.512, ..., 1304.65, 1296.15, 1286.75],\n",
       "       [4, 1706.280029, 2.501, ..., 1296.15, 1286.75, 1283.75],\n",
       "       [4, 1707.6700440000004, 2.519, ..., 1286.75, 1283.75, 1276.1]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train[2:,1:] # drop date\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Time-Series Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_cross(array, time_shift = 0, size_train_set = 365, size_val_set = 100, set_shift = 0):\n",
    "    np.random.seed(100)\n",
    "    label_idx = 0\n",
    "\n",
    "    if time_shift == 0:\n",
    "        time_shift = size_train_set + size_val_set + set_shift\n",
    "        \n",
    "    train_size = train.shape\n",
    "    j = 0\n",
    "    group = 1\n",
    "    end = train_size[0]\n",
    "    trainDataPartition = [] # list\n",
    "    valDataPartition = []\n",
    "\n",
    "    #print(size_train_set, size_val_set, train_size[0])\n",
    "    while j + (size_train_set + size_val_set + set_shift) < train_size[0]:\n",
    "        trainset = []\n",
    "        valset = []\n",
    "\n",
    "        trainset = train[j:j+size_train_set, :] # array\n",
    "        valset = train[j+size_train_set+1+set_shift:j+size_train_set+size_val_set+set_shift, :]\n",
    "\n",
    "        trainDataPartition.append(trainset)\n",
    "        valDataPartition.append(valset)\n",
    "    \n",
    "        group = group+1;\n",
    "        j = j + time_shift\n",
    "\n",
    "\n",
    "    # make last set -- decide if you want to throw out or adjust this set\n",
    "    print('Last set:', j, j+size_train_set)\n",
    "    trainset = train[j:j+size_train_set, :]\n",
    "    valset = train[j+size_train_set+1+set_shift:end, :]\n",
    "    #print(trainset)\n",
    "    #print(valset)\n",
    "    \n",
    "    trainDataPartition.append(trainset)\n",
    "    valDataPartition.append(valset)\n",
    "\n",
    "    \n",
    "    ### Optional: stratify\n",
    "    revised_trainDataPartition = []\n",
    "    revised_valDataPartition = []\n",
    "    \n",
    "    for i in range(len(trainDataPartition)):\n",
    "        trainset = trainDataPartition[i]\n",
    "        valset = valDataPartition[i] # Do the same for validation set\n",
    "        \n",
    "        print(trainset.shape[0], valset.shape[0])\n",
    "        records = trainset[:,0] # record of labels for training set\n",
    "        records_val = valset[:, 0] # record of labels for validation set\n",
    "        #print(sum(records == 1), sum(records == 0))\n",
    "        #print(sum(records_val == 1), sum(records_val == 0))\n",
    "        \n",
    "        ############ FOR TRAINING SET ################\n",
    "        if sum(records == 1) == 0 or sum(records == 0) == 0:\n",
    "            revised_trainDataPartition.append(np.nan)\n",
    "        elif sum(records == 1) > sum(records == 0):\n",
    "            while sum(records == 1) >= 1.3 * sum(records == 0):\n",
    "                r = round(np.random.rand() * (trainset.shape[0]-1))\n",
    "                #print(r)\n",
    "                if records[r] == 1:\n",
    "                    trainset = np.delete(trainset, r, 0)\n",
    "                    records = np.delete(records, r)\n",
    "\n",
    "            revised_trainDataPartition.append(trainset)\n",
    "            \n",
    "        else:\n",
    "            while sum(records == 1) <= 1.3 * sum(records == 0):\n",
    "                r = round(np.random.rand() * (trainset.shape[0]-1))\n",
    "                if records[r] == 0:\n",
    "                    trainset = np.delete(trainset, r, 0)\n",
    "                    records = np.delete(records, r)\n",
    "\n",
    "            revised_trainDataPartition.append(trainset)\n",
    "        #print(sum(records == 1), sum(records == 0))\n",
    "        \n",
    "        ############## FOR VALIDATION SET ################\n",
    "        if sum(records_val == 1) == 0 or sum(records_val == 0) == 0:\n",
    "            revised_valDataPartition.append(np.nan)\n",
    "        elif sum(records_val == 1) > sum(records_val == 0):\n",
    "            while sum(records_val == 1) >= 1.3 * sum(records_val == 0):\n",
    "                r = round(np.random.rand() * (valset.shape[0]-1))\n",
    "                #print(r)\n",
    "                if records_val[r] == 1:\n",
    "                    valset = np.delete(valset, r, 0)\n",
    "                    records_val = np.delete(records_val, r)\n",
    "            revised_valDataPartition.append(valset)\n",
    "        else:\n",
    "            while sum(records_val == 1) <= 1.3 * sum(records_val == 0):\n",
    "                r = round(np.random.rand() * (valset.shape[0]-1))\n",
    "                if records_val[r] == 0:\n",
    "                    valset = np.delete(valset, r, 0)\n",
    "                    records_val = np.delete(records_val, r)\n",
    "\n",
    "            revised_valDataPartition.append(valset)\n",
    "\n",
    "    group = len(revised_trainDataPartition)\n",
    "    group_val = len(revised_valDataPartition)\n",
    "    print('Number of groups:', group, group_val)\n",
    "    \n",
    "    return revised_trainDataPartition, revised_valDataPartition, group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last set: 7000 7410\n",
      "410 259\n",
      "410 259\n",
      "410 259\n",
      "410 259\n",
      "410 259\n",
      "410 259\n",
      "410 259\n",
      "410 259\n",
      "410 259\n",
      "410 259\n",
      "118 0\n",
      "Number of groups: 11 11\n"
     ]
    }
   ],
   "source": [
    "trainDataPartition, valDataPartition, group = time_cross(train, 0, 410, 260, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1625.99</td>\n",
       "      <td>3.013</td>\n",
       "      <td>0.0967244</td>\n",
       "      <td>0.237052</td>\n",
       "      <td>0.280927</td>\n",
       "      <td>1643.9</td>\n",
       "      <td>2.992</td>\n",
       "      <td>0.102957</td>\n",
       "      <td>0.233003</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>252.723</td>\n",
       "      <td>101.081</td>\n",
       "      <td>1239.25</td>\n",
       "      <td>1236.15</td>\n",
       "      <td>1236.45</td>\n",
       "      <td>1241.2</td>\n",
       "      <td>1246.8</td>\n",
       "      <td>1248.25</td>\n",
       "      <td>1244.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1643.9</td>\n",
       "      <td>2.992</td>\n",
       "      <td>0.102957</td>\n",
       "      <td>0.233003</td>\n",
       "      <td>0.31346</td>\n",
       "      <td>1589.08</td>\n",
       "      <td>2.924</td>\n",
       "      <td>0.0916904</td>\n",
       "      <td>0.232878</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>252.673</td>\n",
       "      <td>100.878</td>\n",
       "      <td>1236.45</td>\n",
       "      <td>1241.2</td>\n",
       "      <td>1246.8</td>\n",
       "      <td>1248.25</td>\n",
       "      <td>1244.75</td>\n",
       "      <td>1244.45</td>\n",
       "      <td>1239.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1589.08</td>\n",
       "      <td>2.924</td>\n",
       "      <td>0.0916904</td>\n",
       "      <td>0.232878</td>\n",
       "      <td>0.268168</td>\n",
       "      <td>1587.18</td>\n",
       "      <td>2.876</td>\n",
       "      <td>0.0940545</td>\n",
       "      <td>0.231482</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>252.673</td>\n",
       "      <td>100.878</td>\n",
       "      <td>1241.2</td>\n",
       "      <td>1246.8</td>\n",
       "      <td>1248.25</td>\n",
       "      <td>1244.75</td>\n",
       "      <td>1244.45</td>\n",
       "      <td>1239.15</td>\n",
       "      <td>1239.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1587.18</td>\n",
       "      <td>2.876</td>\n",
       "      <td>0.0940545</td>\n",
       "      <td>0.231482</td>\n",
       "      <td>0.282072</td>\n",
       "      <td>1550.49</td>\n",
       "      <td>2.85</td>\n",
       "      <td>0.0884258</td>\n",
       "      <td>0.23027</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>252.673</td>\n",
       "      <td>100.878</td>\n",
       "      <td>1246.8</td>\n",
       "      <td>1248.25</td>\n",
       "      <td>1244.75</td>\n",
       "      <td>1244.45</td>\n",
       "      <td>1239.15</td>\n",
       "      <td>1239.1</td>\n",
       "      <td>1248.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1550.49</td>\n",
       "      <td>2.85</td>\n",
       "      <td>0.0884258</td>\n",
       "      <td>0.23027</td>\n",
       "      <td>0.260242</td>\n",
       "      <td>1551.95</td>\n",
       "      <td>2.856</td>\n",
       "      <td>0.0974823</td>\n",
       "      <td>0.227566</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>252.673</td>\n",
       "      <td>100.878</td>\n",
       "      <td>1248.25</td>\n",
       "      <td>1244.75</td>\n",
       "      <td>1244.45</td>\n",
       "      <td>1239.15</td>\n",
       "      <td>1239.1</td>\n",
       "      <td>1248.8</td>\n",
       "      <td>1248.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1551.95</td>\n",
       "      <td>2.856</td>\n",
       "      <td>0.0974823</td>\n",
       "      <td>0.227566</td>\n",
       "      <td>0.302867</td>\n",
       "      <td>1550.85</td>\n",
       "      <td>2.879</td>\n",
       "      <td>0.0971161</td>\n",
       "      <td>0.227415</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>252.673</td>\n",
       "      <td>100.878</td>\n",
       "      <td>1244.75</td>\n",
       "      <td>1244.45</td>\n",
       "      <td>1239.15</td>\n",
       "      <td>1239.1</td>\n",
       "      <td>1248.8</td>\n",
       "      <td>1248.6</td>\n",
       "      <td>1255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1550.85</td>\n",
       "      <td>2.879</td>\n",
       "      <td>0.0971161</td>\n",
       "      <td>0.227415</td>\n",
       "      <td>0.300447</td>\n",
       "      <td>1560.05</td>\n",
       "      <td>2.906</td>\n",
       "      <td>0.0989676</td>\n",
       "      <td>0.227266</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>252.673</td>\n",
       "      <td>100.878</td>\n",
       "      <td>1244.45</td>\n",
       "      <td>1239.15</td>\n",
       "      <td>1239.1</td>\n",
       "      <td>1248.8</td>\n",
       "      <td>1248.6</td>\n",
       "      <td>1255</td>\n",
       "      <td>1257.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1560.05</td>\n",
       "      <td>2.906</td>\n",
       "      <td>0.0989676</td>\n",
       "      <td>0.227266</td>\n",
       "      <td>0.307602</td>\n",
       "      <td>1556.78</td>\n",
       "      <td>2.911</td>\n",
       "      <td>0.0973353</td>\n",
       "      <td>0.22633</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>252.673</td>\n",
       "      <td>100.878</td>\n",
       "      <td>1239.15</td>\n",
       "      <td>1239.1</td>\n",
       "      <td>1248.8</td>\n",
       "      <td>1248.6</td>\n",
       "      <td>1255</td>\n",
       "      <td>1257.6</td>\n",
       "      <td>1261.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1556.78</td>\n",
       "      <td>2.911</td>\n",
       "      <td>0.0973353</td>\n",
       "      <td>0.22633</td>\n",
       "      <td>0.301441</td>\n",
       "      <td>1528.36</td>\n",
       "      <td>2.891</td>\n",
       "      <td>0.0875553</td>\n",
       "      <td>0.225287</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>252.673</td>\n",
       "      <td>100.878</td>\n",
       "      <td>1239.1</td>\n",
       "      <td>1248.8</td>\n",
       "      <td>1248.6</td>\n",
       "      <td>1255</td>\n",
       "      <td>1257.6</td>\n",
       "      <td>1261.25</td>\n",
       "      <td>1266.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1528.36</td>\n",
       "      <td>2.891</td>\n",
       "      <td>0.0875553</td>\n",
       "      <td>0.225287</td>\n",
       "      <td>0.260313</td>\n",
       "      <td>1495.61</td>\n",
       "      <td>2.857</td>\n",
       "      <td>0.0862477</td>\n",
       "      <td>0.222839</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>252.673</td>\n",
       "      <td>100.878</td>\n",
       "      <td>1248.8</td>\n",
       "      <td>1248.6</td>\n",
       "      <td>1255</td>\n",
       "      <td>1257.6</td>\n",
       "      <td>1261.25</td>\n",
       "      <td>1266.17</td>\n",
       "      <td>1268.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  0        1      2          3         4         5        6      7    \\\n",
       "0   1  1625.99  3.013  0.0967244  0.237052  0.280927   1643.9  2.992   \n",
       "1   1   1643.9  2.992   0.102957  0.233003   0.31346  1589.08  2.924   \n",
       "2   1  1589.08  2.924  0.0916904  0.232878  0.268168  1587.18  2.876   \n",
       "3   1  1587.18  2.876  0.0940545  0.231482  0.282072  1550.49   2.85   \n",
       "4   1  1550.49   2.85  0.0884258   0.23027  0.260242  1551.95  2.856   \n",
       "5   1  1551.95  2.856  0.0974823  0.227566  0.302867  1550.85  2.879   \n",
       "6   1  1550.85  2.879  0.0971161  0.227415  0.300447  1560.05  2.906   \n",
       "7   1  1560.05  2.906  0.0989676  0.227266  0.307602  1556.78  2.911   \n",
       "8   1  1556.78  2.911  0.0973353   0.22633  0.301441  1528.36  2.891   \n",
       "9   1  1528.36  2.891  0.0875553  0.225287  0.260313  1495.61  2.857   \n",
       "\n",
       "         8         9     ...          137      138      139      140      141  \\\n",
       "0   0.102957  0.233003   ...            1  252.723  101.081  1239.25  1236.15   \n",
       "1  0.0916904  0.232878   ...            1  252.673  100.878  1236.45   1241.2   \n",
       "2  0.0940545  0.231482   ...            1  252.673  100.878   1241.2   1246.8   \n",
       "3  0.0884258   0.23027   ...            1  252.673  100.878   1246.8  1248.25   \n",
       "4  0.0974823  0.227566   ...            1  252.673  100.878  1248.25  1244.75   \n",
       "5  0.0971161  0.227415   ...            1  252.673  100.878  1244.75  1244.45   \n",
       "6  0.0989676  0.227266   ...            1  252.673  100.878  1244.45  1239.15   \n",
       "7  0.0973353   0.22633   ...     0.999999  252.673  100.878  1239.15   1239.1   \n",
       "8  0.0875553  0.225287   ...     0.999999  252.673  100.878   1239.1   1248.8   \n",
       "9  0.0862477  0.222839   ...     0.999999  252.673  100.878   1248.8   1248.6   \n",
       "\n",
       "       142      143      144      145      146  \n",
       "0  1236.45   1241.2   1246.8  1248.25  1244.75  \n",
       "1   1246.8  1248.25  1244.75  1244.45  1239.15  \n",
       "2  1248.25  1244.75  1244.45  1239.15   1239.1  \n",
       "3  1244.75  1244.45  1239.15   1239.1   1248.8  \n",
       "4  1244.45  1239.15   1239.1   1248.8   1248.6  \n",
       "5  1239.15   1239.1   1248.8   1248.6     1255  \n",
       "6   1239.1   1248.8   1248.6     1255   1257.6  \n",
       "7   1248.8   1248.6     1255   1257.6  1261.25  \n",
       "8   1248.6     1255   1257.6  1261.25  1266.17  \n",
       "9     1255   1257.6  1261.25  1266.17  1268.64  \n",
       "\n",
       "[10 rows x 147 columns]"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = pd.DataFrame(valDataPartition[14])\n",
    "t.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1517.5</td>\n",
       "      <td>2.298</td>\n",
       "      <td>0.109504</td>\n",
       "      <td>0.546905</td>\n",
       "      <td>0.158207</td>\n",
       "      <td>1519.13</td>\n",
       "      <td>2.339</td>\n",
       "      <td>0.109717</td>\n",
       "      <td>0.546547</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>247.332</td>\n",
       "      <td>101.299</td>\n",
       "      <td>1289.7</td>\n",
       "      <td>1280.65</td>\n",
       "      <td>1283.4</td>\n",
       "      <td>1280.25</td>\n",
       "      <td>1275.25</td>\n",
       "      <td>1278.3</td>\n",
       "      <td>1273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1527.19</td>\n",
       "      <td>2.381</td>\n",
       "      <td>0.101669</td>\n",
       "      <td>0.545233</td>\n",
       "      <td>0.142799</td>\n",
       "      <td>1520.39</td>\n",
       "      <td>2.376</td>\n",
       "      <td>0.096762</td>\n",
       "      <td>0.543778</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>247.332</td>\n",
       "      <td>101.299</td>\n",
       "      <td>1280.25</td>\n",
       "      <td>1275.25</td>\n",
       "      <td>1278.3</td>\n",
       "      <td>1273</td>\n",
       "      <td>1278</td>\n",
       "      <td>1267.8</td>\n",
       "      <td>1272.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1522.88</td>\n",
       "      <td>2.406</td>\n",
       "      <td>0.0971804</td>\n",
       "      <td>0.543502</td>\n",
       "      <td>0.134536</td>\n",
       "      <td>1515.43</td>\n",
       "      <td>2.444</td>\n",
       "      <td>0.0954212</td>\n",
       "      <td>0.543249</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>247.332</td>\n",
       "      <td>101.299</td>\n",
       "      <td>1278.3</td>\n",
       "      <td>1273</td>\n",
       "      <td>1278</td>\n",
       "      <td>1267.8</td>\n",
       "      <td>1272.75</td>\n",
       "      <td>1274.4</td>\n",
       "      <td>1279.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1518.02</td>\n",
       "      <td>2.452</td>\n",
       "      <td>0.0965311</td>\n",
       "      <td>0.542864</td>\n",
       "      <td>0.13265</td>\n",
       "      <td>1529.83</td>\n",
       "      <td>2.428</td>\n",
       "      <td>0.0946069</td>\n",
       "      <td>0.542391</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>247.332</td>\n",
       "      <td>101.299</td>\n",
       "      <td>1278</td>\n",
       "      <td>1267.8</td>\n",
       "      <td>1272.75</td>\n",
       "      <td>1274.4</td>\n",
       "      <td>1279.25</td>\n",
       "      <td>1276.4</td>\n",
       "      <td>1275.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1523.63</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.0880171</td>\n",
       "      <td>0.540508</td>\n",
       "      <td>0.118994</td>\n",
       "      <td>1526.5</td>\n",
       "      <td>2.376</td>\n",
       "      <td>0.0885293</td>\n",
       "      <td>0.540334</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>247.332</td>\n",
       "      <td>101.299</td>\n",
       "      <td>1274.4</td>\n",
       "      <td>1279.25</td>\n",
       "      <td>1276.4</td>\n",
       "      <td>1275.3</td>\n",
       "      <td>1271.6</td>\n",
       "      <td>1276.35</td>\n",
       "      <td>1282.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1527.51</td>\n",
       "      <td>2.376</td>\n",
       "      <td>0.0886327</td>\n",
       "      <td>0.540159</td>\n",
       "      <td>0.120099</td>\n",
       "      <td>1527.92</td>\n",
       "      <td>2.347</td>\n",
       "      <td>0.0887528</td>\n",
       "      <td>0.539345</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>247.332</td>\n",
       "      <td>101.299</td>\n",
       "      <td>1276.4</td>\n",
       "      <td>1275.3</td>\n",
       "      <td>1271.6</td>\n",
       "      <td>1276.35</td>\n",
       "      <td>1282.25</td>\n",
       "      <td>1284</td>\n",
       "      <td>1284.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1532.27</td>\n",
       "      <td>2.343</td>\n",
       "      <td>0.0908523</td>\n",
       "      <td>0.538541</td>\n",
       "      <td>0.125194</td>\n",
       "      <td>1534.62</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.0884516</td>\n",
       "      <td>0.536555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>247.901</td>\n",
       "      <td>101.217</td>\n",
       "      <td>1271.6</td>\n",
       "      <td>1276.35</td>\n",
       "      <td>1282.25</td>\n",
       "      <td>1284</td>\n",
       "      <td>1284.45</td>\n",
       "      <td>1278.4</td>\n",
       "      <td>1273.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1532.14</td>\n",
       "      <td>2.307</td>\n",
       "      <td>0.0877951</td>\n",
       "      <td>0.536378</td>\n",
       "      <td>0.120671</td>\n",
       "      <td>1534.45</td>\n",
       "      <td>2.325</td>\n",
       "      <td>0.0881815</td>\n",
       "      <td>0.536198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>247.901</td>\n",
       "      <td>101.217</td>\n",
       "      <td>1282.25</td>\n",
       "      <td>1284</td>\n",
       "      <td>1284.45</td>\n",
       "      <td>1278.4</td>\n",
       "      <td>1273.7</td>\n",
       "      <td>1285.7</td>\n",
       "      <td>1277.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1534.45</td>\n",
       "      <td>2.325</td>\n",
       "      <td>0.0881815</td>\n",
       "      <td>0.536198</td>\n",
       "      <td>0.121096</td>\n",
       "      <td>1528.4</td>\n",
       "      <td>2.331</td>\n",
       "      <td>0.0857761</td>\n",
       "      <td>0.53537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>247.901</td>\n",
       "      <td>101.217</td>\n",
       "      <td>1284</td>\n",
       "      <td>1284.45</td>\n",
       "      <td>1278.4</td>\n",
       "      <td>1273.7</td>\n",
       "      <td>1285.7</td>\n",
       "      <td>1277.7</td>\n",
       "      <td>1283.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1528.4</td>\n",
       "      <td>2.331</td>\n",
       "      <td>0.0857761</td>\n",
       "      <td>0.53537</td>\n",
       "      <td>0.116678</td>\n",
       "      <td>1527.49</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0853463</td>\n",
       "      <td>0.534553</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>247.901</td>\n",
       "      <td>101.217</td>\n",
       "      <td>1284.45</td>\n",
       "      <td>1278.4</td>\n",
       "      <td>1273.7</td>\n",
       "      <td>1285.7</td>\n",
       "      <td>1277.7</td>\n",
       "      <td>1283.85</td>\n",
       "      <td>1292.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  0        1      2          3         4         5        6      7    \\\n",
       "0   1   1517.5  2.298   0.109504  0.546905  0.158207  1519.13  2.339   \n",
       "1   1  1527.19  2.381   0.101669  0.545233  0.142799  1520.39  2.376   \n",
       "2   1  1522.88  2.406  0.0971804  0.543502  0.134536  1515.43  2.444   \n",
       "3   1  1518.02  2.452  0.0965311  0.542864   0.13265  1529.83  2.428   \n",
       "4   1  1523.63   2.37  0.0880171  0.540508  0.118994   1526.5  2.376   \n",
       "5   1  1527.51  2.376  0.0886327  0.540159  0.120099  1527.92  2.347   \n",
       "6   1  1532.27  2.343  0.0908523  0.538541  0.125194  1534.62   2.32   \n",
       "7   1  1532.14  2.307  0.0877951  0.536378  0.120671  1534.45  2.325   \n",
       "8   1  1534.45  2.325  0.0881815  0.536198  0.121096   1528.4  2.331   \n",
       "9   1   1528.4  2.331  0.0857761   0.53537  0.116678  1527.49    2.4   \n",
       "\n",
       "         8         9     ...          137      138      139      140      141  \\\n",
       "0   0.109717  0.546547   ...            1  247.332  101.299   1289.7  1280.65   \n",
       "1   0.096762  0.543778   ...            1  247.332  101.299  1280.25  1275.25   \n",
       "2  0.0954212  0.543249   ...            1  247.332  101.299   1278.3     1273   \n",
       "3  0.0946069  0.542391   ...            1  247.332  101.299     1278   1267.8   \n",
       "4  0.0885293  0.540334   ...            1  247.332  101.299   1274.4  1279.25   \n",
       "5  0.0887528  0.539345   ...            1  247.332  101.299   1276.4   1275.3   \n",
       "6  0.0884516  0.536555   ...     0.999999  247.901  101.217   1271.6  1276.35   \n",
       "7  0.0881815  0.536198   ...     0.999999  247.901  101.217  1282.25     1284   \n",
       "8  0.0857761   0.53537   ...     0.999999  247.901  101.217     1284  1284.45   \n",
       "9  0.0853463  0.534553   ...            1  247.901  101.217  1284.45   1278.4   \n",
       "\n",
       "       142      143      144      145      146  \n",
       "0   1283.4  1280.25  1275.25   1278.3     1273  \n",
       "1   1278.3     1273     1278   1267.8  1272.75  \n",
       "2     1278   1267.8  1272.75   1274.4  1279.25  \n",
       "3  1272.75   1274.4  1279.25   1276.4   1275.3  \n",
       "4   1276.4   1275.3   1271.6  1276.35  1282.25  \n",
       "5   1271.6  1276.35  1282.25     1284  1284.45  \n",
       "6  1282.25     1284  1284.45   1278.4   1273.7  \n",
       "7  1284.45   1278.4   1273.7   1285.7   1277.7  \n",
       "8   1278.4   1273.7   1285.7   1277.7  1283.85  \n",
       "9   1273.7   1285.7   1277.7  1283.85  1292.35  \n",
       "\n",
       "[10 rows x 147 columns]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trainDataPartition[18]\n",
    "t = pd.DataFrame(trainDataPartition[14])\n",
    "t.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest Implementation\n",
    "No normalization necessary for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "### Try Random Forest Classifier\n",
    "\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf(train_list, val_list, group):\n",
    "    splits = group\n",
    "    score = []\n",
    "\n",
    "    #kf = sklearn.model_selection.KFold(n_splits=splits, random_state = 10, shuffle = True)\n",
    "    #kf.get_n_splits(features)\n",
    "\n",
    "    data_size = trainDataPartition[0].shape\n",
    "\n",
    "    for idx in range(len(trainDataPartition)-1):\n",
    "            # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "            try:\n",
    "                X_train, y_train = trainDataPartition[idx][:,1:data_size[1]+1], trainDataPartition[idx][:,0]\n",
    "                y_train = y_train.astype('int')\n",
    "                #print(X_train)\n",
    "                X_test, y_test = valDataPartition[idx][:,1:data_size[1]+1], valDataPartition[idx][:,0]\n",
    "                y_test = y_test.astype('int')\n",
    "                print('train:', sum(y_train), len(y_train))\n",
    "                print('test:', sum(y_test), len(y_test))\n",
    "\n",
    "                # Fit the RF model\n",
    "                clf = RandomForestClassifier(n_estimators=50, max_depth=1500, random_state=0)\n",
    "                clf.fit(X_train, y_train)\n",
    "                \n",
    "                # print predicitions\n",
    "                pred = clf.predict(X_test)\n",
    "                #print(pred)\n",
    "\n",
    "            except:\n",
    "                print('Skipped due to NaN')\n",
    "                continue # nan\n",
    "                \n",
    "            # add up AUROCs            \n",
    "            try:\n",
    "                temp_score = sklearn.metrics.roc_auc_score(y_test, pred)\n",
    "                #temp_score = sklearn.metrics.accuracy_score(y_test,pred)\n",
    "                #score = score + sklearn.metrics.accuracy_score(y_test, pred)\n",
    "                #temp_score = sum([1 for i in range(len(pred)) if pred[i] == y_test[i]]) / len(pred)\n",
    "                score.append(temp_score)\n",
    "                print(temp_score)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "    # calculate average\n",
    "    score = np.mean(score)\n",
    "    print(\"Averaged Score is: %0.4f\" % score, splits)\n",
    "\n",
    "\n",
    "    # print(clf.feature_importances_)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groups 15\n",
      "train: 136 241\n",
      "test: 98 173\n",
      "Skipped due to NaN\n",
      "train: 124 220\n",
      "test: 61 107\n",
      "0.5\n",
      "train: 94 167\n",
      "test: 103 183\n",
      "Skipped due to NaN\n",
      "train: 141 252\n",
      "test: 77 136\n",
      "0.50198106977768\n",
      "train: 111 197\n",
      "test: 100 199\n",
      "Skipped due to NaN\n",
      "train: 93 164\n",
      "test: 45 79\n",
      "0.4993464052287582\n",
      "train: 113 199\n",
      "test: 80 142\n",
      "0.48004032258064516\n",
      "train: 119 211\n",
      "test: 100 199\n",
      "0.5\n",
      "train: 122 216\n",
      "test: 95 168\n",
      "0.4098774333093006\n",
      "train: 102 180\n",
      "test: 103 183\n",
      "0.4967839805825244\n",
      "train: 142 252\n",
      "test: 83 147\n",
      "0.5947853915662651\n",
      "train: 67 118\n",
      "test: 97 172\n",
      "0.4997250859106529\n",
      "train: 136 241\n",
      "test: 98 173\n",
      "0.576530612244898\n",
      "train: 104 183\n",
      "test: 90 159\n",
      "0.5\n",
      "Averaged Score is: 0.5054 15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.505370027381884"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('groups', group)\n",
    "rf(valDataPartition, trainDataPartition, group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] 100 50\n",
      "56 44\n",
      "19 30\n",
      "48 52\n",
      "18 31\n",
      "59 41\n",
      "28 21\n",
      "69 31\n",
      "7 42\n",
      "24 76\n",
      "28 21\n",
      "21 79\n",
      "49 0\n",
      "73 27\n",
      "31 18\n",
      "42 58\n",
      "41 8\n",
      "59 41\n",
      "43 6\n",
      "44 56\n",
      "42 7\n",
      "16 84\n",
      "48 1\n",
      "66 34\n",
      "30 19\n",
      "63 37\n",
      "25 24\n",
      "21 79\n",
      "11 38\n",
      "41 59\n",
      "17 32\n",
      "21 79\n",
      "10 39\n",
      "50 50\n",
      "19 30\n",
      "51 49\n",
      "39 10\n",
      "57 43\n",
      "29 20\n",
      "53 47\n",
      "44 5\n",
      "63 37\n",
      "44 5\n",
      "19 81\n",
      "36 13\n",
      "57 43\n",
      "28 21\n",
      "50 50\n",
      "26 23\n",
      "37 63\n",
      "30 19\n",
      "62 38\n",
      "44 5\n",
      "42 58\n",
      "9 40\n",
      "54 46\n",
      "42 7\n",
      "48 52\n",
      "46 3\n",
      "61 39\n",
      "14 35\n",
      "22 78\n",
      "5 44\n",
      "74 26\n",
      "30 19\n",
      "59 41\n",
      "43 6\n",
      "33 67\n",
      "46 3\n",
      "29 71\n",
      "9 40\n",
      "56 44\n",
      "0 49\n",
      "48 52\n",
      "15 34\n",
      "62 38\n",
      "15 34\n",
      "97 3\n",
      "22 27\n",
      "43 35\n",
      "0 0\n",
      "Number of groups: 40 40\n",
      "train: 56 100\n",
      "test: 19 33\n",
      "Skipped due to NaN\n",
      "train: 48 84\n",
      "test: 18 31\n",
      "0.5\n",
      "train: 53 94\n",
      "test: 27 48\n",
      "Skipped due to NaN\n",
      "train: 40 71\n",
      "test: 7 12\n",
      "0.5\n",
      "train: 24 42\n",
      "test: 27 48\n",
      "0.5\n",
      "Skipped due to NaN\n",
      "train: 35 62\n",
      "test: 23 41\n",
      "Skipped due to NaN\n",
      "train: 42 74\n",
      "test: 10 18\n",
      "Skipped due to NaN\n",
      "train: 53 94\n",
      "test: 7 13\n",
      "0.5\n",
      "train: 44 77\n",
      "test: 9 16\n",
      "0.5\n",
      "train: 16 28\n",
      "test: 1 2\n",
      "0.0\n",
      "train: 44 78\n",
      "test: 24 43\n",
      "0.6041666666666666\n",
      "train: 48 85\n",
      "test: 25 49\n",
      "0.6958333333333333\n",
      "train: 21 37\n",
      "test: 11 19\n",
      "Skipped due to NaN\n",
      "train: 41 72\n",
      "test: 17 30\n",
      "0.5067873303167421\n",
      "train: 21 37\n",
      "test: 10 17\n",
      "0.5\n",
      "train: 50 88\n",
      "test: 19 33\n",
      "0.4530075187969924\n",
      "train: 51 100\n",
      "test: 12 22\n",
      "0.625\n",
      "train: 55 98\n",
      "test: 25 45\n",
      "0.325\n",
      "train: 53 100\n",
      "test: 6 11\n",
      "0.4166666666666667\n",
      "train: 48 85\n",
      "test: 6 11\n",
      "0.2833333333333334\n",
      "train: 19 33\n",
      "test: 16 29\n",
      "0.31009615384615385\n",
      "train: 55 98\n",
      "test: 27 48\n",
      "0.7486772486772486\n",
      "train: 50 88\n",
      "test: 26 49\n",
      "0.5\n",
      "train: 37 65\n",
      "test: 24 43\n",
      "0.26754385964912286\n",
      "train: 49 87\n",
      "test: 6 11\n",
      "0.5\n",
      "train: 42 74\n",
      "test: 9 15\n",
      "0.5\n",
      "train: 54 100\n",
      "test: 9 16\n",
      "0.5\n",
      "train: 48 84\n",
      "test: 3 6\n",
      "0.5\n",
      "train: 50 89\n",
      "test: 14 24\n",
      "0.5\n",
      "train: 22 38\n",
      "test: 5 8\n",
      "0.9\n",
      "train: 33 59\n",
      "test: 24 43\n",
      "0.6688596491228069\n",
      "train: 53 94\n",
      "test: 7 13\n",
      "0.5\n",
      "train: 33 58\n",
      "test: 3 6\n",
      "0.8333333333333333\n",
      "train: 29 51\n",
      "test: 9 15\n",
      "0.8055555555555555\n",
      "Skipped due to NaN\n",
      "train: 48 84\n",
      "test: 15 26\n",
      "0.5575757575757576\n",
      "train: 49 87\n",
      "test: 15 26\n",
      "0.5\n",
      "train: 3 6\n",
      "test: 22 38\n",
      "0.5\n",
      "Averaged Score is: 0.5157 40\n",
      "[0] 100 100\n",
      "56 44\n",
      "56 43\n",
      "14 86\n",
      "59 40\n",
      "66 34\n",
      "84 15\n",
      "15 85\n",
      "40 59\n",
      "41 59\n",
      "78 21\n",
      "58 42\n",
      "44 55\n",
      "73 27\n",
      "68 31\n",
      "41 59\n",
      "54 45\n",
      "34 66\n",
      "74 25\n",
      "50 50\n",
      "71 28\n",
      "29 71\n",
      "22 77\n",
      "44 56\n",
      "15 84\n",
      "28 72\n",
      "43 56\n",
      "51 49\n",
      "61 38\n",
      "73 27\n",
      "57 42\n",
      "50 50\n",
      "79 20\n",
      "67 33\n",
      "32 67\n",
      "58 42\n",
      "75 24\n",
      "50 50\n",
      "43 56\n",
      "25 75\n",
      "61 38\n",
      "76 24\n",
      "54 45\n",
      "32 68\n",
      "82 17\n",
      "35 65\n",
      "72 27\n",
      "29 71\n",
      "31 68\n",
      "51 49\n",
      "58 41\n",
      "53 47\n",
      "63 36\n",
      "56 44\n",
      "25 74\n",
      "46 54\n",
      "45 54\n",
      "35 65\n",
      "44 55\n",
      "67 33\n",
      "72 27\n",
      "60 40\n",
      "32 35\n",
      "Number of groups: 31 31\n",
      "train: 56 100\n",
      "test: 55 98\n",
      "Skipped due to NaN\n",
      "train: 14 24\n",
      "test: 51 91\n",
      "Skipped due to NaN\n",
      "train: 44 78\n",
      "test: 19 34\n",
      "0.23684210526315788\n",
      "train: 15 26\n",
      "test: 40 70\n",
      "0.5\n",
      "train: 41 72\n",
      "test: 27 48\n",
      "0.5\n",
      "train: 54 96\n",
      "test: 44 77\n",
      "Skipped due to NaN\n",
      "train: 35 62\n",
      "test: 40 71\n",
      "0.517741935483871\n",
      "train: 41 72\n",
      "test: 54 99\n",
      "0.48148148148148145\n",
      "train: 34 60\n",
      "test: 32 57\n",
      "0.546875\n",
      "train: 50 88\n",
      "test: 36 64\n",
      "0.47817460317460314\n",
      "train: 29 51\n",
      "test: 22 38\n",
      "Skipped due to NaN\n",
      "train: 44 77\n",
      "test: 15 26\n",
      "0.4666666666666667\n",
      "train: 28 49\n",
      "test: 43 76\n",
      "0.5\n",
      "train: 51 100\n",
      "test: 49 87\n",
      "0.41702470461868957\n",
      "train: 35 62\n",
      "test: 54 96\n",
      "0.4232804232804232\n",
      "train: 50 88\n",
      "test: 25 45\n",
      "0.5\n",
      "train: 42 75\n",
      "test: 32 56\n",
      "0.5364583333333333\n",
      "train: 54 96\n",
      "test: 31 55\n",
      "0.5\n",
      "train: 50 88\n",
      "test: 43 76\n",
      "0.5\n",
      "train: 25 44\n",
      "test: 49 87\n",
      "0.4876476906552095\n",
      "train: 31 55\n",
      "test: 54 99\n",
      "0.5962962962962963\n",
      "train: 32 56\n",
      "test: 22 39\n",
      "0.5227272727272727\n",
      "train: 35 61\n",
      "test: 35 62\n",
      "0.6132275132275132\n",
      "train: 29 51\n",
      "test: 31 54\n",
      "0.5\n",
      "train: 51 100\n",
      "test: 53 94\n",
      "0.5\n",
      "train: 53 100\n",
      "test: 46 82\n",
      "0.5537439613526569\n",
      "train: 56 100\n",
      "test: 25 44\n",
      "0.5905263157894737\n",
      "train: 46 81\n",
      "test: 45 79\n",
      "0.5\n",
      "train: 35 61\n",
      "test: 44 77\n",
      "0.5454545454545454\n",
      "train: 42 75\n",
      "test: 35 62\n",
      "0.5\n",
      "Averaged Score is: 0.5005 31\n",
      "[0] 100 260\n",
      "56 44\n",
      "93 166\n",
      "81 19\n",
      "125 134\n",
      "34 66\n",
      "170 89\n",
      "54 46\n",
      "152 107\n",
      "61 39\n",
      "122 137\n",
      "78 22\n",
      "154 105\n",
      "21 79\n",
      "95 164\n",
      "21 79\n",
      "121 138\n",
      "61 39\n",
      "143 116\n",
      "72 28\n",
      "124 135\n",
      "65 35\n",
      "129 130\n",
      "42 58\n",
      "158 101\n",
      "42 58\n",
      "146 113\n",
      "36 64\n",
      "103 156\n",
      "38 62\n",
      "158 101\n",
      "83 17\n",
      "138 121\n",
      "42 58\n",
      "102 157\n",
      "54 46\n",
      "160 99\n",
      "43 35\n",
      "0 0\n",
      "Number of groups: 19 19\n",
      "train: 56 100\n",
      "test: 93 164\n",
      "Skipped due to NaN\n",
      "train: 24 43\n",
      "test: 125 221\n",
      "Skipped due to NaN\n",
      "train: 34 60\n",
      "test: 115 204\n",
      "0.5\n",
      "train: 54 100\n",
      "test: 139 246\n",
      "Skipped due to NaN\n",
      "train: 50 89\n",
      "test: 122 215\n",
      "0.5629737352370879\n",
      "train: 28 50\n",
      "test: 136 241\n",
      "0.7435924369747899\n",
      "train: 21 37\n",
      "test: 95 168\n",
      "Skipped due to NaN\n",
      "train: 21 37\n",
      "test: 121 214\n",
      "0.46027725939749403\n",
      "train: 50 89\n",
      "test: 143 259\n",
      "0.5627863515794551\n",
      "train: 36 64\n",
      "test: 124 219\n",
      "0.5256366723259762\n",
      "train: 45 80\n",
      "test: 129 228\n",
      "0.5\n",
      "train: 42 74\n",
      "test: 131 232\n",
      "0.6800317436323785\n",
      "train: 42 74\n",
      "test: 146 259\n",
      "0.5\n",
      "train: 36 63\n",
      "test: 103 182\n",
      "0.558252427184466\n",
      "train: 38 67\n",
      "test: 131 232\n",
      "0.5\n",
      "train: 22 39\n",
      "test: 138 259\n",
      "0.7256258234519104\n",
      "train: 42 74\n",
      "test: 102 180\n",
      "0.4875565610859729\n",
      "train: 54 100\n",
      "test: 128 227\n",
      "0.5\n",
      "Averaged Score is: 0.5576 19\n",
      "[0] 252 50\n",
      "147 105\n",
      "2 47\n",
      "146 106\n",
      "49 0\n",
      "78 174\n",
      "11 38\n",
      "189 63\n",
      "27 22\n",
      "156 96\n",
      "13 36\n",
      "105 147\n",
      "48 1\n",
      "158 94\n",
      "37 12\n",
      "64 188\n",
      "29 20\n",
      "70 182\n",
      "18 31\n",
      "145 107\n",
      "35 14\n",
      "132 120\n",
      "37 12\n",
      "127 125\n",
      "42 7\n",
      "139 113\n",
      "22 27\n",
      "101 151\n",
      "40 9\n",
      "123 129\n",
      "41 8\n",
      "153 99\n",
      "42 7\n",
      "64 188\n",
      "40 9\n",
      "149 103\n",
      "24 25\n",
      "134 118\n",
      "15 34\n",
      "111 141\n",
      "9 40\n",
      "163 89\n",
      "21 28\n",
      "65 61\n",
      "0 0\n",
      "Number of groups: 22 22\n",
      "train: 136 241\n",
      "test: 2 3\n",
      "Skipped due to NaN\n",
      "Skipped due to NaN\n",
      "train: 78 137\n",
      "test: 11 19\n",
      "0.5625\n",
      "train: 81 144\n",
      "test: 27 49\n",
      "0.5\n",
      "train: 124 220\n",
      "test: 13 22\n",
      "0.5\n",
      "train: 105 185\n",
      "test: 1 2\n",
      "0.5\n",
      "train: 122 216\n",
      "test: 15 27\n",
      "0.08333333333333331\n",
      "train: 64 113\n",
      "test: 25 45\n",
      "Skipped due to NaN\n",
      "train: 70 123\n",
      "test: 18 31\n",
      "0.5\n",
      "train: 139 246\n",
      "test: 18 32\n",
      "0.6865079365079365\n",
      "train: 132 252\n",
      "test: 15 27\n",
      "0.5\n",
      "train: 127 252\n",
      "test: 9 16\n",
      "0.5\n",
      "train: 139 252\n",
      "test: 22 38\n",
      "0.5\n",
      "train: 101 178\n",
      "test: 11 20\n",
      "0.5\n",
      "train: 123 217\n",
      "test: 10 18\n",
      "0.15\n",
      "train: 128 227\n",
      "test: 9 16\n",
      "0.7222222222222222\n",
      "train: 64 113\n",
      "test: 11 20\n",
      "0.5\n",
      "train: 133 236\n",
      "test: 24 42\n",
      "0.41666666666666663\n",
      "train: 134 252\n",
      "test: 15 26\n",
      "0.9545454545454545\n",
      "train: 111 196\n",
      "test: 9 15\n",
      "0.4166666666666667\n",
      "train: 115 204\n",
      "test: 21 37\n",
      "0.5\n",
      "Averaged Score is: 0.4996 22\n",
      "[0] 252 100\n",
      "147 105\n",
      "22 77\n",
      "176 76\n",
      "25 74\n",
      "89 163\n",
      "78 21\n",
      "131 121\n",
      "70 29\n",
      "146 106\n",
      "13 86\n",
      "176 76\n",
      "71 28\n",
      "67 185\n",
      "53 46\n",
      "69 183\n",
      "29 70\n",
      "181 71\n",
      "66 33\n",
      "157 95\n",
      "67 32\n",
      "133 119\n",
      "64 35\n",
      "72 180\n",
      "61 38\n",
      "158 94\n",
      "56 43\n",
      "158 94\n",
      "48 51\n",
      "83 169\n",
      "66 33\n",
      "157 95\n",
      "75 24\n",
      "95 157\n",
      "28 71\n",
      "122 130\n",
      "54 45\n",
      "133 89\n",
      "0 0\n",
      "Number of groups: 19 19\n",
      "train: 136 241\n",
      "test: 22 38\n",
      "Skipped due to NaN\n",
      "train: 98 174\n",
      "test: 25 44\n",
      "Skipped due to NaN\n",
      "train: 89 157\n",
      "test: 27 48\n",
      "0.5185185185185185\n",
      "train: 131 252\n",
      "test: 37 66\n",
      "Skipped due to NaN\n",
      "train: 137 243\n",
      "test: 13 22\n",
      "0.5\n",
      "train: 98 174\n",
      "test: 36 64\n",
      "0.4861111111111111\n",
      "train: 67 118\n",
      "test: 53 99\n",
      "Skipped due to NaN\n",
      "train: 69 122\n",
      "test: 29 51\n",
      "0.5\n",
      "train: 92 163\n",
      "test: 42 75\n",
      "0.4166666666666667\n",
      "train: 123 218\n",
      "test: 41 73\n",
      "0.6429115853658537\n",
      "train: 133 252\n",
      "test: 45 80\n",
      "0.5\n",
      "train: 72 127\n",
      "test: 49 87\n",
      "0.5\n",
      "train: 122 216\n",
      "test: 55 98\n",
      "0.5\n",
      "train: 122 216\n",
      "test: 48 84\n",
      "0.5729166666666666\n",
      "train: 83 146\n",
      "test: 42 75\n",
      "0.5\n",
      "train: 123 218\n",
      "test: 31 55\n",
      "0.8125\n",
      "train: 95 168\n",
      "test: 28 49\n",
      "0.45833333333333326\n",
      "train: 122 215\n",
      "test: 54 99\n",
      "0.5\n",
      "Averaged Score is: 0.5291 19\n",
      "[0] 252 260\n",
      "147 105\n",
      "137 122\n",
      "117 135\n",
      "143 116\n",
      "152 100\n",
      "183 76\n",
      "110 142\n",
      "176 83\n",
      "126 126\n",
      "96 163\n",
      "79 173\n",
      "155 104\n",
      "140 112\n",
      "181 78\n",
      "112 140\n",
      "135 124\n",
      "101 151\n",
      "129 130\n",
      "152 100\n",
      "125 134\n",
      "118 134\n",
      "170 89\n",
      "127 125\n",
      "101 158\n",
      "123 129\n",
      "164 95\n",
      "17 35\n",
      "0 0\n",
      "Number of groups: 14 14\n",
      "train: 136 241\n",
      "test: 137 259\n",
      "Skipped due to NaN\n",
      "train: 117 206\n",
      "test: 143 259\n",
      "0.2902097902097902\n",
      "train: 129 229\n",
      "test: 98 174\n",
      "Skipped due to NaN\n",
      "train: 110 194\n",
      "test: 107 190\n",
      "0.48524940885035467\n",
      "train: 126 222\n",
      "test: 96 169\n",
      "Skipped due to NaN\n",
      "train: 79 139\n",
      "test: 135 239\n",
      "0.524928774928775\n",
      "train: 140 252\n",
      "test: 101 179\n",
      "0.5044427519675044\n",
      "train: 112 198\n",
      "test: 135 259\n",
      "0.5148148148148148\n",
      "train: 101 178\n",
      "test: 129 228\n",
      "0.6381254404510219\n",
      "train: 129 229\n",
      "test: 125 221\n",
      "0.6507916666666667\n",
      "train: 118 208\n",
      "test: 115 204\n",
      "0.5130434782608696\n",
      "train: 127 252\n",
      "test: 101 178\n",
      "0.624919634820625\n",
      "train: 123 217\n",
      "test: 123 218\n",
      "0.5\n",
      "Averaged Score is: 0.5247 14\n",
      "[0] 410 50\n",
      "192 218\n",
      "43 6\n",
      "200 210\n",
      "8 41\n",
      "268 142\n",
      "42 7\n",
      "212 198\n",
      "42 7\n",
      "245 165\n",
      "14 35\n",
      "129 281\n",
      "25 24\n",
      "226 184\n",
      "39 10\n",
      "235 175\n",
      "31 18\n",
      "234 176\n",
      "18 31\n",
      "232 178\n",
      "20 29\n",
      "261 149\n",
      "14 35\n",
      "190 220\n",
      "42 7\n",
      "205 205\n",
      "22 27\n",
      "199 211\n",
      "25 24\n",
      "149 89\n",
      "0 0\n",
      "Number of groups: 15 15\n",
      "train: 192 339\n",
      "test: 7 13\n",
      "Skipped due to NaN\n",
      "train: 200 353\n",
      "test: 8 14\n",
      "0.6875\n",
      "train: 184 326\n",
      "test: 9 16\n",
      "Skipped due to NaN\n",
      "train: 212 410\n",
      "test: 9 16\n",
      "0.3333333333333333\n",
      "train: 214 379\n",
      "test: 14 24\n",
      "0.5642857142857143\n",
      "train: 129 228\n",
      "test: 25 49\n",
      "0.5\n",
      "train: 226 410\n",
      "test: 12 22\n",
      "0.4583333333333333\n",
      "train: 227 402\n",
      "test: 23 41\n",
      "0.5\n",
      "train: 228 404\n",
      "test: 18 31\n",
      "0.5\n",
      "train: 231 409\n",
      "test: 20 35\n",
      "0.7083333333333333\n",
      "train: 193 342\n",
      "test: 14 24\n",
      "0.8428571428571427\n",
      "train: 190 336\n",
      "test: 9 16\n",
      "0.5873015873015872\n",
      "train: 205 362\n",
      "test: 22 38\n",
      "0.5\n",
      "train: 199 352\n",
      "test: 25 49\n",
      "0.5\n",
      "Averaged Score is: 0.5568 15\n",
      "[0] 410 100\n",
      "192 218\n",
      "72 27\n",
      "177 233\n",
      "92 7\n",
      "247 163\n",
      "67 32\n",
      "202 208\n",
      "54 45\n",
      "162 248\n",
      "41 58\n",
      "153 257\n",
      "74 25\n",
      "256 154\n",
      "66 33\n",
      "226 184\n",
      "35 64\n",
      "216 194\n",
      "34 65\n",
      "246 164\n",
      "18 81\n",
      "200 210\n",
      "85 14\n",
      "198 212\n",
      "28 71\n",
      "227 183\n",
      "45 54\n",
      "43 35\n",
      "0 0\n",
      "Number of groups: 14 14\n",
      "train: 192 339\n",
      "test: 35 62\n",
      "Skipped due to NaN\n",
      "train: 177 313\n",
      "test: 9 16\n",
      "0.16666666666666666\n",
      "train: 211 374\n",
      "test: 41 73\n",
      "Skipped due to NaN\n",
      "train: 202 357\n",
      "test: 54 99\n",
      "0.6740740740740739\n",
      "train: 162 286\n",
      "test: 41 72\n",
      "Skipped due to NaN\n",
      "train: 153 270\n",
      "test: 32 57\n",
      "0.534375\n",
      "train: 200 354\n",
      "test: 42 75\n",
      "0.5\n",
      "train: 226 410\n",
      "test: 35 61\n",
      "0.2967032967032967\n",
      "train: 216 410\n",
      "test: 34 60\n",
      "0.6990950226244345\n",
      "train: 213 377\n",
      "test: 18 31\n",
      "0.6688034188034189\n",
      "train: 200 353\n",
      "test: 18 32\n",
      "0.6865079365079365\n",
      "train: 198 350\n",
      "test: 28 49\n",
      "0.5\n",
      "train: 227 410\n",
      "test: 45 79\n",
      "0.5\n",
      "Averaged Score is: 0.5226 14\n",
      "[0] 410 260\n",
      "192 218\n",
      "176 83\n",
      "194 216\n",
      "137 122\n",
      "262 148\n",
      "146 113\n",
      "190 220\n",
      "103 156\n",
      "183 227\n",
      "150 109\n",
      "235 175\n",
      "171 88\n",
      "174 236\n",
      "130 129\n",
      "261 149\n",
      "83 176\n",
      "256 154\n",
      "132 127\n",
      "190 220\n",
      "174 85\n",
      "49 49\n",
      "0 0\n",
      "Number of groups: 11 11\n",
      "train: 192 339\n",
      "test: 107 190\n",
      "Skipped due to NaN\n",
      "train: 194 343\n",
      "test: 137 259\n",
      "Skipped due to NaN\n",
      "train: 192 340\n",
      "test: 146 259\n",
      "0.5689477512425748\n",
      "train: 190 336\n",
      "test: 103 182\n",
      "Skipped due to NaN\n",
      "train: 183 323\n",
      "test: 141 250\n",
      "0.5690025375756393\n",
      "train: 227 402\n",
      "test: 114 202\n",
      "0.5206339712918661\n",
      "train: 174 307\n",
      "test: 130 259\n",
      "0.6595706618962434\n",
      "train: 193 342\n",
      "test: 83 146\n",
      "0.5302161025052592\n",
      "train: 200 354\n",
      "test: 132 259\n",
      "0.5385946074922453\n",
      "train: 190 336\n",
      "test: 110 195\n",
      "0.61524064171123\n",
      "Averaged Score is: 0.5717 11\n",
      "[[2 1 2]]\n",
      "[[[0.51566989 0.50054496 0.55762379]\n",
      "  [0.49958013 0.52913985 0.52465258]\n",
      "  [0.5568287  0.52262254 0.57174375]]]\n",
      "[[(100,  50) (100, 100) (100, 260)]\n",
      " [(252,  50) (252, 100) (252, 260)]\n",
      " [(410,  50) (410, 100) (410, 260)]]\n"
     ]
    }
   ],
   "source": [
    "### Test\n",
    "# time_shift, size_train_set, size_val_set\n",
    "\n",
    "time_shift = [0] # smaller time shift is better, smaller training set, smaller val set 0, 50, 100, 126, 252, 504\n",
    "# 504 252 50 0.61268998\n",
    "size_train_set = [100, 252, 410]\n",
    "size_val_set = [50, 100, 260] # predict a month ahead (shift labels)\n",
    "\n",
    "score_matrix = np.zeros((len(time_shift), len(size_train_set), len(size_val_set)))\n",
    "settings_matrix = np.zeros((len(size_train_set), len(size_val_set)), dtype = 'i,i')\n",
    "\n",
    "for i in range(len(time_shift)):\n",
    "    for j in range(len(size_train_set)):\n",
    "        for k in range(len(size_val_set)):\n",
    "            print(time_shift, size_train_set[j], size_val_set[k])\n",
    "            trainDataPartition, valDataPartition, group = time_cross(train, time_shift[i], size_train_set[j], size_val_set[k], 30)\n",
    "\n",
    "            #time_shift[i]\n",
    "            try:\n",
    "                score_matrix[i][j][k] = rf(valDataPartition, trainDataPartition, group)\n",
    "            except:\n",
    "                print('Cannot compute score')\n",
    "            settings_matrix[j][k][0] = size_train_set[j]\n",
    "            settings_matrix[j][k][1] = size_val_set[k]\n",
    "            \n",
    "print(np.argmax(score_matrix, axis = 1))\n",
    "print(score_matrix)\n",
    "print(settings_matrix)\n",
    "#0.61268998 Didn't work so well with Label 2\n",
    "# Perfomed well with predicting next day with Google Trends 0.8975, best was 0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5568287 , 0.52913985, 0.57174375]])"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.max(score_matrix, axis = 1)\n",
    "#print(np.argmax(t, axis = 1))\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look for Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192 218\n",
      "176 83\n",
      "194 216\n",
      "137 122\n",
      "262 148\n",
      "146 113\n",
      "190 220\n",
      "103 156\n",
      "183 227\n",
      "150 109\n",
      "235 175\n",
      "171 88\n",
      "174 236\n",
      "130 129\n",
      "261 149\n",
      "83 176\n",
      "256 154\n",
      "132 127\n",
      "190 220\n",
      "174 85\n",
      "49 49\n",
      "0 0\n",
      "Number of groups: 11 11\n"
     ]
    }
   ],
   "source": [
    "trainDataPartition, valDataPartition, group = time_cross(train, 0, 410, 260, 30) #10 252 100 Draft_Google_shorter 0.8795"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 192 339\n",
      "test: 107 190\n",
      "Skipped\n",
      "train: 194 343\n",
      "test: 137 259\n",
      "Skipped\n",
      "train: 192 340\n",
      "test: 146 259\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1]\n",
      "0.5689477512425748\n",
      "train: 190 336\n",
      "test: 103 182\n",
      "Skipped\n",
      "train: 183 323\n",
      "test: 141 250\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "0.5690025375756393\n",
      "train: 227 402\n",
      "test: 114 202\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0]\n",
      "0.5206339712918661\n",
      "train: 174 307\n",
      "test: 130 259\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "0.6595706618962434\n",
      "train: 193 342\n",
      "test: 83 146\n",
      "[0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1]\n",
      "0.5302161025052592\n",
      "train: 200 354\n",
      "test: 132 259\n",
      "[0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "0.5385946074922453\n",
      "train: 190 336\n",
      "test: 110 195\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 0 0 0]\n",
      "0.61524064171123\n",
      "Skipped\n",
      "Averaged Score is: 0.5717 11\n"
     ]
    }
   ],
   "source": [
    "splits = group\n",
    "score_list = []\n",
    "indices = []\n",
    "pred_labels = []\n",
    "\n",
    "data_size = trainDataPartition[0].shape\n",
    "\n",
    "for idx in range(len(trainDataPartition)):\n",
    "    try:\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, y_train = trainDataPartition[idx][:,1:data_size[1]+1], trainDataPartition[idx][:,0]\n",
    "        y_train = y_train.astype('int')\n",
    "        #print(X_train)\n",
    "        X_test, y_test = valDataPartition[idx][:,1:data_size[1]+1], valDataPartition[idx][:,0]\n",
    "        y_test = y_test.astype('int')\n",
    "        print('train:', sum(y_train), len(y_train))\n",
    "        print('test:', sum(y_test), len(y_test))\n",
    "    \n",
    "        # Fit the RF model\n",
    "        clf = RandomForestClassifier(n_estimators=50, max_depth=1500, random_state=0) # previously 7\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # print predicitions\n",
    "        pred = clf.predict(X_test)\n",
    "        pred_labels.append(pred)\n",
    "        print(pred)\n",
    "        #print(pred)\n",
    "    except:\n",
    "        print('Skipped')\n",
    "        continue #np.nan\n",
    "        \n",
    "    # add up AUROCs\n",
    "            \n",
    "    try:\n",
    "        temp_score = sklearn.metrics.roc_auc_score(y_test, pred)\n",
    "        score_list.append(temp_score)\n",
    "        indices.append(idx)\n",
    "        print(temp_score)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# calculate average\n",
    "score = np.mean(score_list)\n",
    "print(\"Averaged Score is: %0.4f\" % score, splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the accuracy through time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Deviation: 0.04637810217797863\n",
      "[2, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xt4XOV16P/v0s2yrLsl32WNDTLYxsY2kiGhkARyMSExBE4SnKQp7ZOQc3IIyckJLaQ0acih6S1tfj2l+ZWQlNAGEwIEDDgYSJNASEASGt+N8X108UW2ZyRL1l3r/LH30GEsWSNpRnsu6/M881iz552914ylNXvWu9/3FVXFGGNMZsjyOgBjjDFTx5K+McZkEEv6xhiTQSzpG2NMBrGkb4wxGcSSvjHGZBBL+iZuROT/iMhJETnmdSwmPkTkVhH57Xke/7WIfG4qYzKTY0k/g4nIYRHpEZEuETkuIv8mIoUT3FcV8L+BZao6J76Rmslyk/eQ+38dvv2z13GZqWdJ33xUVQuBNUAdcM94dyAiOUA1cEpVT0zw+RlrCl//71W1MOJ2+xQd1yQRS/oGAFVtBX4BXAIgIiUi8kMROSoirW7pJtt97FYReVVE/lFETgO/Bl4E5rlnkA+57daLyC4RCbllgKXh47nfMv5MRLYD3SKS4267U0S2i0i3e/zZIvILETkjIi+JSFnEPn4mIsdEpENEXhaR5RGPPSQi94vIc+5zXxeRCyIeXy4iL4rIafdbztfd7VkicpeIHBCRUyLymIiUj/SeiUiZiDwrIu0iEnR/XhDxeLn77anNffwpd/t7RaTFff3HgH9zt39eRPa7MW0SkXnudnHf6xPua90uIuH/pw+LyG73NbaKyNfG+3/v/l8/7L6OIyJyj4iMmBtE5AMi8qYbxz8DEvHYhSLyG/exkyLy0/HGYhLPkr4B3i7PfBjwu5t+DAwCFwKrgQ8CkbXby4GDwCzgA8B1QJt7BnmriCwBNgJfASqBzcAzIpIXsY8NwPVAqaoOuttudve3BPgozgfR14EKnN/XOyKe/wugxo2hCfhJ1MvaAHwLKAP2A/e5r7UIeAl4HpjnvsZfus+5A7gReI/7WBC4f5S3LQsnYVcDC4EeILJk8u9AAbDcjfEfIx6bA5S7z71NRK4BvgN8ApgLHAEeddt+ELjafU9KgU8Cp9zHfgh8QVWLcD6w/3OUWM/n/wIlwGKc1/1Z4I+jG4lIBfAEzrfBCuAAcGVEk28DL+C83wvc/Zpko6p2y9AbcBjoAkI4SeZfgOnAbKAPmB7RdgPwK/fnW4FA1L7eC7RE3P8L4LGI+1lAK/DeiGP/yQjxfDri/hPA9yPufwl4apTXUgooUOLefwh4MOLxDwNvRrwW/yj72QNcG3F/LjAA5MTwfq4CghHPGwbKRmj3XqAfyI/Y9kPgbyPuF7rH9QHXAG8BVwBZUfsKAF8AiseI7VacD/FQxO0KINv9v14W0fYLwK8jnvdb9+fPAq9FtBOgBfice/9h4AFggde/23Yb/WZn+uZGVS1V1WpV/aKq9uCcfeYCR93STAj4V5yz1bDmMfY7D+eDBABVHXafM3+MfRyP+LlnhPuFACKSLSJ/7ZZhOnE+MMA5Aw2LvIrobPi5QBXOWepIqoGfR7zuPcAQzgfhO4hIgYj8q1sS6QReBkrdMlgVcFpVg6Mcp11VeyPuR79fXThn8/NV9T9xvkHcDxwXkQdEpNhtejPOB9oRt7TyrlGOB07CLo24vYbzfuVFHtv9ef4Iz59HxP+ZOpk+8v/wT3E+COrdst6fnCcW4xFL+mYkzThnfxURCaJYVZdHtBlretY2nAQKOHVpnETYOo59nM+ngBuA9+OUJnzhQ8Xw3GbggvM8dl1UcsxXp88j2v8GLgIuV9VinBJMOIZmoFxESkc5TvRrj36/ZgAzcd8vVf0nVb0Mp1S0BLjT3d6gqjfgfCA/BTx2ntc9kpM43yiqI7Yt5J3/T2FHcf4PwzFK5H1VPaaqn1fVeTjfFv5FRC4cZzwmwSzpm3Oo6lGc2ux3RaTY7dy8QETeM47dPAZcLyLXikguToLsA34XpzCL3P2dwqmb/9U4nvssMEdEviIi00SkSEQudx/7/4H7RKQaQEQqReSG88TQA4Tczt5vhh9w38Nf4CS+MhHJFZGrR9kPwCPAH4vIKhGZ5r6e11X1sIjUicjl7vvYDfQCQyKSJyKfFpESVR0AOnG+lcRMVYdw/q/uc9+HauCrwH+M0Pw5YLmI3CTOFUd34PRNACAiH4/oyA7ifLCNKx6TeJb0zWg+i/O1fzfOH/DjOHXqmKjqXuAzOJ15J3E6ZT+qqv1xiu9hnDJEqxvja+OI7QxOZ/FHcUpA+4D3uQ//f8Am4AUROePu9/KR9gN8D6cP5KTb7vmox/8Q5yz6TeAETqf2aDH9Eqcf5AmcM+oLgFvch4uBH+D8PxzB+aD7+4hjHHbLS/8d5z0fry/hfJgcBH6L8wH0oxFiPAl8HPhrN4Ya4NWIJnXA6yLShfMefllVD00gHpNA4pTljDHGZAI70zfGmAxiSd8YYzKIJX1jjMkglvSNMSaDJN1EVxUVFerz+bwOwxhjUsobb7xxUlUrx2qXdEnf5/PR2NjodRjGGJNSROTI2K2svGOMMRnFkr4xxmQQS/rGGJNBLOkbY0wGiSnpi8g6Ednrrupz1yhtPuGu4LNLRB6J2L5QRF4QkT3u4774hG6MMWa8xrx6x50b/H6cCapagAYR2aSquyPa1AB3A1eqalBEIuddfxi4T1VfFGfR7eG4vgJjjDExi+VMfy2wX1UPujMkPoozj3mkzwP3hxeMUHdxbBFZhrPi0Ivu9i5VPRu36I0xxoxLLEl/Pu9cHaeFc1fVWQIsEWex7NdEZF3E9pCIPCkifhH5O/ebgzEGGB5WftoQ4Gz/4NiNjYmDWJL+SCsRRc/HnIMzt/Z7cdYffdBdMSgHuAr4Gs5c24tx1tx85wFEbhORRhFpbG9vjzl4Y1LdG4Egf/bEDn7W2OJ1KCZDxJL0W4hYEg1nlfu2Edo8raoD7qIJe3E+BFpwFqA+qKqDOMu5rYk+gKo+oKq1qlpbWTnmKGJj0kbTEWcJ3YbDpz2OxGSKWJJ+A1AjIotEJA9nNZ9NUW2ewl15SEQqcMo6B93nlolIOJNfg7PKkTEG8AdCgJP0bUEjMxXGTPruGfrtwBZgD/CYqu4SkXtFZL3bbAtwSkR2A78C7lTVU+76m18DfikiO3BKRT9IxAsxJtWoKk2BIHk5WRzv7KMl2ON1SCYDxDThmqpuBjZHbftGxM+Ks5jyV0d47ovAysmFaUz6OdrRy4kzfWxYu5CN9QHqD52mqrzA67BMmrMRucZ4pCng1PM/WVdFcX4OjUesrm8Sz5K+MR7xB0JMy8li+bxian3l1B+ypG8Sz5K+MR7xB4KsXFBCbnYWtb4yDrR3c6qrz+uwTJqzpG+MB/oGh9jZ2snqhWUArPWVA9DoXsJpTKJY0jfGA7vbOukfGmbNwlIAViwoIS8ni0a7Xt8kmCV9YzwQvj4/fKY/LSebVQtKqT9sZ/omsSzpG+OBpkCQeSX5zC7Of3tbra+MXa0dNg+PSShL+sZ4wB8Isbq67B3b6nzlDA4rW5tDHkVlMoElfWOm2InOXlpDPayuKn3H9jXVZYhAwyEr8ZjEsaRvzBRriqrnh5VMz+Wi2UU2SMsklCV9Y6aYvzlIbrawfF7xOY+tXVRO05Egg0O2wJxJDEv6xkwxfyDE8nkl5Oeeu55Qra+c7v4h9hw940FkJhNY0jdmCg0ODbO9JcTqhaUjPl7nc0o+9Xa9vkkQS/rGTKE3j52hd2D4nHp+2NyS6Swom26DtEzCWNI3Zgr53Zk114xypg/OlAy2qIpJFEv6xkwhfyBEZdE05pdOH7VNra+ck139HD51dgojM5nCkr4xU6gpEGR1VSkiMmqbtYuc0k+DTbVsEsCSvjFT5HS3c/a+pnrken7YBZWFlBXk2mLpJiEs6RszRbY2O/X86JG40USEWreub0y8WdI3Zoo0HQmRnSWsWFAyZts6XxmHT53lxJneKYjMZJKYkr6IrBORvSKyX0TuGqXNJ0Rkt4jsEpFHoh4rFpFWEfnneARtTCryNwdZOreIgrycMdvWhRdVsamWTZyNmfRFJBu4H7gOWAZsEJFlUW1qgLuBK1V1OfCVqN18G/hNXCI2JgUNDSvbmjtYXXX+en6YM2I3y0o8Ju5iOdNfC+xX1YOq2g88CtwQ1ebzwP2qGgRQ1RPhB0TkMmA28EJ8QjYm9ew7cYauvsFRR+JGy8vJYnVVmSV9E3exJP35QHPE/RZ3W6QlwBIReVVEXhORdQAikgV8F7jzfAcQkdtEpFFEGtvb22OP3pgUEb1SVizqfGXsbuukq88WVTHxE0vSH+mC4uihgjlADfBeYAPwoIiUAl8ENqtqM+ehqg+oaq2q1lZWVsYQkjGpxR8IUlaQi29mQczPqVtUzrBCky2WbuJo7B4l58y+KuL+AqBthDavqeoAcEhE9uJ8CLwLuEpEvggUAnki0qWqI3YGG5OumgIhVi8sO++grGirF5aRJdB4+DRXL7GTIRMfsZzpNwA1IrJIRPKAW4BNUW2eAt4HICIVOOWeg6r6aVVdqKo+4GvAw5bwTabp6Blg/4muMa/Pj1Y4LYfl80psxk0TV2MmfVUdBG4HtgB7gMdUdZeI3Csi691mW4BTIrIb+BVwp6qeSlTQxqSSbe6at2ONxB1Jra+Mrc0h+gdtURUTH7GUd1DVzcDmqG3fiPhZga+6t9H28RDw0ESCNCaV+QMhRGBlDIOyoq31lfNvrx5mZ1sHa8bRCWzMaGxErjEJ1hQIsmRWEUX5ueN+7mXuoio2v76JF0v6xiTQ8LCytTnEmurx1fPDZhXl45tZQP0hu4LHxIclfWMS6NCpbjp6BmIeiTuSOl85bxw5zfCwLapiJs+SvjEJFL7GPtaRuCOp85UTPDvAwZNd8QrLZDBL+sYkkL85RFF+DhdUFk54H3WLnMnXrMRj4sGSvjEJ5A+EWFVVSlZW7IOyovlmFlBRmGeduSYuLOkbkyBdfYPsPdY5rvl2RiIi1PnKbZCWiQtL+sYkyPaWEMMKayZRzw+r9ZXTEuzhaEdPHCIzmcySvjEJEp5Zc9U4p18YyVp3UZUGW1TFTJIlfWMSxB8IsrhyBqUFeZPe19K5RczIy7a6vpk0S/rGJICq4g+EJnV9fqSc7CzWVJdRf8iSvpkcS/rGJEDz6R5OdfdPeCTuSGqry9l7/AwdPQNx26fJPJb0jUmApoA7KCtOZ/oAdYvKUFtUxUySJX1jEsAfCFKQl82S2RMflBVtdVUZOVli6+aaSbGkb0wC+JtDXLqglJzs+P2JTc/L5pL5JZb0zaRY0jcmznoHhtjd1jmp+XZGU+crY1tzB70DQ3Hft8kMlvSNibMdrR0MDuukR+KOpM5XTv/QMDtaO+K+b5MZLOkbE2f+wORn1hxN7duDtKzEYybGkr4xceYPhFhYXkBF4bS477t8Rh4Xziqkwa7XNxMUU9IXkXUisldE9ovIXaO0+YSI7BaRXSLyiLttlYj83t22XUQ+Gc/gjUk2qkpTIJiQs/ywOl8ZjUeCtqiKmZAxk76IZAP3A9cBy4ANIrIsqk0NcDdwpaouB77iPnQW+Ky7bR3wPRFJ3F+DMR472tHL8c6+hC5iXucr50zvIHuPn0nYMUz6iuVMfy2wX1UPqmo/8ChwQ1SbzwP3q2oQQFVPuP++par73J/bgBNAZbyCNybZhCdZS+yZvlPXt3l4zETEkvTnA80R91vcbZGWAEtE5FUReU1E1kXvRETWAnnAgYkGa0yyawoEmZaTxcVzihN2jAVl05lTnE+9zbhpJiAnhjYjLfkTXUzMAWqA9wILgFdE5BJVDQGIyFzg34E/UtXhcw4gchtwG8DChQtjDt6YZOMPBFkxv4S8nMRdIyEi1PrKaDh0GlVFZOKrcpnME8tvZgtQFXF/AdA2QpunVXVAVQ8Be3E+BBCRYuA54B5VfW2kA6jqA6paq6q1lZVW/TGpqW9wiJ1tnaypTlw9P2ztonKOdfbSErRFVcz4xJL0G4AaEVkkInnALcCmqDZPAe8DEJEKnHLPQbf9z4GHVfVn8QvbmOSzu62T/sFhVsdh0ZSx1Fa7df0jVtc34zNm0lfVQeB2YAuwB3hMVXeJyL0ist5ttgU4JSK7gV8Bd6rqKeATwNXArSKy1b2tSsgrMcZj/9WJm/gz/YvmFFE0LcdW0jLjFktNH1XdDGyO2vaNiJ8V+Kp7i2zzH8B/TD5MY5KfvznEvJJ85pTkJ/xY2VnCZW5d35jxsBG5xsRJ05HglJzlh9X5ytl3ootgd/+UHdMkzvM7j/HCrmM459CJY0nfmDg40dlLa6gnodfnR3v7en1bVCXlqSp/8/ybPPDywYRfjWVJ35g48DdPXT0/bOWCEvKys2yQVhp47eBpDp3sZsPaxF+ybknfmDjwB0LkZgvL5yVuUFa0/NxsVi4ood6SfsrbWB+gOD+H61fOTfixLOkbEwdNgSDL5pWQn5s9pcet9ZWzs7WDnn5bVCVVne7u5/mdx7hpzYIp+f2xpG/MJA0ODbO9JcSaKaznh61dVMbAkLLVLS+Z1PNkUwv9Q8NTUtoBS/rGTNqbx87QOzA8pfX8sMsWliNik6+lKlXlkfoAl1WXcdGcoik5piV9Yybp7ZWypmAkbrSSglwuml1kdf0UVX/oNAfbp6YDN8ySvjGT5A+EqCicxoKy6Z4cv9ZXRtORIIND58xlaJLcxvoARfk5XL8i8R24YZb0jZkkf7NTz/dqtss6Xznd/UO8ecwWVUklwe5+Nu88xk2r5zM9b+ouALCkb8wknO7u59DJbk/q+WF1tlh6SnqiqYX+wWE2XD6108lb0jdmErY2u/V8D67cCZtXOp35pdMt6acQVWVjfYDVC0sTuuDOSCzpGzMJ/kCI7Cxh5YIST+Oo85XRcDiY8HlbTHw0HA5yYIo7cMMs6RszCU2BIBfPKaIgL6YJaxOmblE57Wf6OHLqrKdxmNhsrA9QNC2Hj0zBCNxolvSNmaChYWVbc4enpZ0wq+unjtDZfp7bcZQbV8/35GTBkr4xE7T/RBddfYOs8bATN+zCykJKC3It6aeAJ5tanQ5cD0o7YEnfmAlrCg/KSoKkn5Ul1FaX0WgraSW1cAfupVWlLJvCyfkiWdI3ZoL8gSBlBbn4ZhZ4HQrglHgOnuym/Uyf16GYUbxxJMi+E1182qOzfLCkb8yE+QMhVi8s82xQVrRat67/hi2WnrQeqQ9QOC2Hj1w69R24YZb0jZmAjp4B9p3o8mS+ndGsmF/CtJws6g9ZiScZdZwd4LntR7lx9TxPr/aKKemLyDoR2Ssi+0XkrlHafEJEdovILhF5JGL7H4nIPvf2R/EK3BgvbfNgpayx5OVksaqqlEY7009KP/e30OdhB27YmElfRLKB+4HrgGXABhFZFtWmBrgbuFJVlwNfcbeXA98ELgfWAt8UkeT5KzFmgvyBECJwaZW3g7Ki1fnK2dXWSXffoNehmAhOB24zly4oYfk8b39nYjnTXwvsV9WDqtoPPArcENXm88D9qhoEUNUT7vYPAS+q6mn3sReBdfEJ3Rjv+JuDLJlVRFF+rtehvEPdonKGhhV/wBZVSSZNgSB7j5/x/CwfYkv684HmiPst7rZIS4AlIvKqiLwmIuvG8VxE5DYRaRSRxvb29tijN8YDw25STYZBWdHWLCwlS2yQVrJ55PVmZuRl89FL53kdSkxJf6RLE6In+MgBaoD3AhuAB0WkNMbnoqoPqGqtqtZWVlbGEJIx3jl0qpuOnoGkTPpF+bksnVtsST+JdJwd4Nntbdywej4zpnk7XQfElvRbgKqI+wuAthHaPK2qA6p6CNiL8yEQy3ONSSnh0kkyjMQdSZ2vHH8gxIAtqpIUntraSt/gMJ9KgtIOxJb0G4AaEVkkInnALcCmqDZPAe8DEJEKnHLPQWAL8EERKXM7cD/objMmZTUFghRNy+GCykKvQxlRna+cnoEhdrV1eh1KxguPwF0xv4RL5idHp/+YSV9VB4HbcZL1HuAxVd0lIveKyHq32RbglIjsBn4F3Kmqp1T1NPBtnA+OBuBed5sxKcsfCLFqYSlZWckxKCtanc/5BmKLpXvP3xzizWPJ0YEbFlOBSVU3A5ujtn0j4mcFvureop/7I+BHkwvTmOTQ3TfI3mOdfOCaGq9DGdWs4nyqZxZQf+g0n7tqsdfhZLSNrweYkZfN+lXed+CG2YhcY8ZhW0uIYfV2paxY1FaX03jEFlXxUmfvAM9sb2P9qvkUJkEHbpglfWPGIdyJm0zTL4xk7aIyTnf3c6C92+tQMtbT/lZ6B5KnAzfMkr4x4+APhFhcOYPSgjyvQzmv8ORrVtf3hqryk9cDXDK/mBUeL6UZzZK+MTFSVfyBIKurkvNSzUiLK2Ywc0Ye9Zb0PbGtpSPpOnDDLOkbE6Pm0z2c6u5P+no+gIhQ67NFVbyy8fUABXnZrE+CEbjRLOkbEyN/s5NAk3VQVrQ6XzmB02c53tnrdSgZ5UzvAJu2tbH+0nlJNzcTWNI3Jmb+QIiCvGyWzE7OQVnRbLF0bzy1tY2egaGkLO2AJX1jYtYUCLJyQQk52anxZ7N8XjEFedk0HLKkP1VUlUdeD7BsbjErk6wDNyw1fnuN8VjvwBC72zqTatGUseRkZ7F6YSkNVtefMttbOthztJMNly9MmmU0o1nSNyYGO1s7GBzWlKnnh9X5ytlzrJPO3gGvQ8kIG+sDTM/N5oYkGoEbzZK+MTFoCjhny6uSfFBWtDpfOarQdMTO9hMt3IH70UvnUpyEHbhhlvSNiYE/EKKqfDqVRdO8DmVcVi8sJTtLrDN3Cmza1sbZ/uTtwA2zpG9MDPyBUMqVdgAK8nK4ZF6x1fWnwMb6AEvnFif9t0FL+saMoS3Uw7HO3qSfb2c0db5ytjaH6Bsc8jqUtLWjpYOdrZ18am1V0nbghlnSN2YMb0+yloJn+uDMw9M/OMzO1g6vQ0lbj9QHyM/N4obV5ywBnnQs6RszBn8gyLScLJbOLfY6lAkJL6pSf8hKPInQ1TfIpq2tfHTlvKTuwA2zpG/MGJoCQVbMLyEvJzX/XGYWTmNx5QybcTNBntnWRnf/EBsuT+4O3LDU/C02Zor0DQ6xs60zJSZZO586d1GV4WFbVCXeNtYHuHhOUcr0+VjSN+Y89hw9Q//gcEpeuROpblE5HT0D7DvR5XUoaWVnawfbWzrYsDZ5R+BGiynpi8g6EdkrIvtF5K4RHr9VRNpFZKt7+1zEY38rIrtEZI+I/JOkyjtjDP81qClVO3HDwnV9u14/vh6pDzAtJ4sbU6ADN2zMpC8i2cD9wHXAMmCDiCwboelPVXWVe3vQfe67gSuBlcAlQB3wnngFb0yi+ZtDzC3JZ05JvtehTMrC8gJmFU2zpB9H3X2DPO1v5SMr51EyPfk7cMNiOdNfC+xX1YOq2g88CtwQ4/4VyAfygGlALnB8IoEa4wV/IJjy9XxwFlWp85XboipxFO7A/dTlVV6HMi6xJP35QHPE/RZ3W7SbRWS7iDwuIlUAqvp74FfAUfe2RVX3RD9RRG4TkUYRaWxvbx/3izAmEU6c6aUl2JPy9fywOl8ZraEeWkM9XoeSFjbWB1gyuzDlfj9iSfoj1eCjLwF4BvCp6krgJeDHACJyIbAUWIDzQXGNiFx9zs5UH1DVWlWtraysHE/8xiTMfw3KSv0zfbDF0uNpZ2sH21KsAzcslqTfAkR+f1kAtEU2UNVTqtrn3v0BcJn788eA11S1S1W7gF8AV0wuZGOmhj8QIjdbWD4vORfDGK+lc4spnJZDvS2qMmmPNjgduDetXuB1KOMWS9JvAGpEZJGI5AG3AJsiG4jI3Ii764FwCScAvEdEckQkF6cT95zyjjHJyB8IsmxeCfm52V6HEhfZWcKaalssfbLO9g/ylL+N61fOpaQgdTpww8ZM+qo6CNwObMFJ2I+p6i4RuVdE1rvN7nAvy9wG3AHc6m5/HDgA7AC2AdtU9Zk4vwZj4m5waJjtLR0pM+AmVmt9Zew9fobQ2X6vQ0lZz247SlffIJ9K8imUR5MTSyNV3Qxsjtr2jYif7wbuHuF5Q8AXJhmjMVPuzWNn6BkYSpt6fli4rv/GkSDXLp3tcTSp6ZH6ADWzCrmsOrU6cMNsRK4xI/A3O524qXZlxlhWVZWSmy3UW2fuhOxu62RrcyglO3DDLOkbMwL/kSAVhdNYUDbd61DiKj83mxXzS6yuP0GPNgTIy8nipjWpMwI3miV9Y0bgbw6xemFpyp7NnU/donK2t4ToHbBFVcajp3+Inze1cv2KuZQW5HkdzoRZ0jcmSrC7n0Mnu9OutBNWV13OwJCyzS1hmdg8u72NM32DSb8G7lgs6RsTxd8cnmQtvTpxw2pt8rUJeaQ+wAWVM96evC5VWdI3Joo/ECI7S1i5ID0GZUUrLchjyexCWyx9HPYc7cQfSO0O3DBL+sZE8QdCXDyniIK8mK5oTkl1vnKajgQZskVVYvJofYC87CxuXpN6I3CjWdI3JsLQsLLV7cRNZ3W+cs70DfLmsU6vQ0l6Pf1DPOlv5boVcyibkboduGGW9I2JsP9EF119g6yuSu267VjqFjmDtBpsHp4xPbfjKGd6U78DNyxtvr929Q3yd8+/+Y5t0V9cdYRvshrVKrrNSF9+z93Pua3O2c85+43hOWMed4T9jNgmej963sdHOlZ+bhZfuqaGqvKCEVqnD38gvTtxw+aXTmdeST4NR4LceuUir8NJahvrAyyunMHl7gdlqkubpN8/OMzT29rO2R7d5TJSJ8y5bcZqcW6bsfcBEtVqrH04bcbuNDpnP+fsdyLxv3NLW6iHQye7+elt7yIrK7U7ss7HHwhRWpDLoooZXoeScHWLyvn9gVOoasp3TibK3mNneONIkHuuX5o271HaJP3yGXls/cYHvQ4jbT3W2MyfPr6dRxua+dTl6fE1dyRNgSCrq9JzUFa0Wl85T29to/l0Dwtnpvc3uIna6Hbg3pQGHbhhVtM3MfleKBcvAAAXK0lEQVT4ZQu4YnE53/nFHk509nodTkJ09Ayw70RXyi+CHqvw9eY2D8/IegeGeLKphXWXzKE8DTpwwyzpm5iICH/1sRX0DQ7zrWd3ex1OQmxvSc9J1kazZFYRxfk5tpLWKDbvOEpnGnXghlnSNzFbXFnIHddcyHPbj/LLPem3vn3TkRAisLIqPQdlRcvKEmp95XamP4qN9QEWVczgisXp0YEbZknfjMttV1/AktmF/MVTO+nuG/Q6nLjyNwepmVVIcX7qrYY0UXW+cg62d3Oqq2/sxhlk3/EzNBwOsmFtVdr171jSN+OSl5PFd25aQVtHL9994S2vw4kbVcUfCGVMaScsXNdvPGJTMkTaWN9MbrakxQjcaJb0zbhdVl3OZ65YyEO/O/R2HTzVHTzZTUfPQNpfnx9txYIS8nKybJBWhN6BIZ5oauFDy+cws3Ca1+HEnSV9MyF/uu5iKgqncdcTOxgcGvY6nEnzB5wPr0y5cidsWk42qxaU0mBn+m/7xc6jdPQMpOwauGOxpG8mpDg/l2+tX87uo5386NVDXoczaf5AkKJpOVxYWeh1KFOublEZu1o7ONufXn00E7Xx9WZ8Mwu4YvFMr0NJiJiSvoisE5G9IrJfRO4a4fFbRaRdRLa6t89FPLZQRF4QkT0isltEfPEL33hp3SVzeP/S2fzDi2/RfPqs1+FMSlMgxKqFpWk92ng0tb5yBoeVrYH0KNVNxv4TZ6g/fJpb1i5M29+FMZO+iGQD9wPXAcuADSKybISmP1XVVe7twYjtDwN/p6pLgbXAiTjEbZKAiHDvDcvJFuGep3aeM6dPqujuG2TvsU5WV2VWPT/ssuoyRGyQFvxXB+5/uyz9OnDDYjnTXwvsV9WDqtoPPArcEMvO3Q+HHFV9EUBVu1Q1tU8JzTvMK53OnR+6iN+81c6mEeY+SgXbWzoY1syr54cV5+dy8ZzijF8sPdyB+8Hlc6hIww7csFiS/nygOeJ+i7st2s0isl1EHheRKnfbEiAkIk+KiF9E/s795vAOInKbiDSKSGN7e/u4X4Tx1h++y8elVaXc+8xuQmf7vQ5n3JrcmTVXZeiZPsBaXxlNgWBadMpP1JZdxwidTd8O3LBYkv5Iha3o7/HPAD5VXQm8BPzY3Z4DXAV8DagDFgO3nrMz1QdUtVZVaysrK2MM3SSL7CzhOx9bQahngL/avMfrcMbNHwixuGJGWiyQMVG1vnLO9g+x+2jmLqryyOsBqmcW8K407cANiyXptwBVEfcXAO/4Hq+qp1Q1PKTvB8BlEc/1u6WhQeApYM3kQjbJaNm8Yj5/1WIea2zh9wdOeR1OzFSVrc1BVmXY9fnR6nzOVAP1GXq9/oH2Ll4/dJpb6tK3AzcslqTfANSIyCIRyQNuATZFNhCRuRF31wN7Ip5bJiLh0/drgPScrcvw5WtrWFhewJ//fAe9A0NehxOTlmAPJ7v6M24kbrQ5JflUlU/P2Lr+o/UBcrLSuwM3bMyk756h3w5swUnmj6nqLhG5V0TWu83uEJFdIrINuAO3hKOqQzilnV+KyA6cUtEP4v8yTDKYnpfNfR+7hIMnu/mXX+33OpyYNGXISlmxqPOV03D4dMpehTVRfYNDPP5GCx9cPpvKovTtwA2LaREVVd0MbI7a9o2In+8G7h7luS8CKycRo0khV9VU8rHV8/n+bw7wkUvnsWR2kdchnZc/EKIgL5uLkjzOqVDnK+fJplYOnexmcQYNUtuy6zjBswNpN4XyaGxErom7e65fyoxpOdz95A6Gh5P7rNEfCLJyQQk52fanEK7rN2TY9fobXw9QVT6dKy+o8DqUKWG/6SbuZhZO457rl/HGkSAbGwJehzOq3oEhdrV1Zuz1+dEuqJxB+Yw8GjKorn+wvYvfHzyVER24YZb0TULcvGY+775gJn+9+U2OJ+nyijtbOxgc1owdiRtNRKitLsuoM/1HG5rJyRI+Xpv+HbhhlvRNQogI931sBX1Dw3zrmV1ehzOiTJ1Z83zqfOUcOXU2bddBjhTuwH3/0tnMKsr3OpwpY0nfJMyiihl8+doaNu84xku7k295xaZAkKry6RlxxUas6haF6/rpX+J5YddxTnf3s+HyzOjADbOkbxLq81ct5qLZRXzj6Z10Jdnyiv5AiNVVdpYfafm8YvJzszKixLOxPsCCsulcdWFmdOCGWdI3CZWXk8Vf3bSCo529fPeFvV6H87ajHT0c6+y16/Oj5GZnsboq/ev6h05287sDp9iQxlMoj8aSvkm4y6rL+Mzl1Tz0u8NsbU6OOdubjjhxZPpI3JHULSpnz9FOzvQOeB1KwjzaECA7S/h4BozAjWZJ30yJO9ddxKyiadz95A4GkmAmR38gSF5OFkvnFnsdStKp85UxrM7CMumof3CYxxtbeP/SWcwqzpwO3DBL+mZKOMsrXsKeo5386LfeL6/obw6xYr6zKLh5p9ULy8jOEhrTtMTz4u7jnOruz5gRuNHsN95MmXWXzOGDy2bzjy+9ReCUd2vp9A8Os6O1gzVWzx9R4bQcls0tTtu6/sb6APNLp3NVTWZO425J30ypb92wnJysLP78qR2eTey1+2gn/YPDdn3+edT5yvEHQvQPel+Ki6cjp7r57f6T3FJXRXaGdeCGWdI3U2puibO84iv7TvL0Vm+WV/TbzJpjqvOV0Tc4zM62Dq9DiatHG5qdDtzaqrEbpylL+mbKfeaKalZVlfLtZ3cT7J765RX9gRBzS/KZWzJ9yo+dKmrDk6+l0aIq/YPD/KyxmWsunsWckszrwA2zpG+mXHaW8J2bVtDh0fKKTYGgneWPobJoGosqZqTVyNyX9hznZFd/2q+BOxZL+sYTS+cW8/mrF/OzN1r43f6TU3bcE2d6aQn22EjcGNT5ymg8cjrpp8eO1cb6APNK8rl6SWZ24IZZ0jee+fK1NVTPLODPn9o5ZcsrbnWvPV9TbWf6Y6n1lRM6O8CB9i6vQ5m0wKmzvLLvJJ+sW5ixHbhhlvSNZ/Jzs7nvxhUcOtnN/VO0vGJTIERutrB8XsmUHC+VrQ0vlp4Gl24+2hAgS+CTdZnbgRtmSd946g9qKrhpzXy+/+sD7D12JuHH8weCLJtbTH5udsKPleqqZxZQUTgt5RdLHxga5rHGFq65eHZGd+CGxZT0RWSdiOwVkf0ictcIj98qIu0istW9fS7q8WIRaRWRf45X4CZ93HP9Moryc7j7ye0JrR8PDg2zvaXDrs+PkYiwdlEZ9Sl+Bc8v9xznZFcfn7rczvIhhqQvItnA/cB1wDJgg4gsG6HpT1V1lXt7MOqxbwO/mXS0Ji2Vz8jjnuuX0RQI8ZP6xC2v+OaxM/QMDNmVO+NQW11Oa6iHtlCP16FM2CP1zcwtyec9S2Z5HUpSiOVMfy2wX1UPqmo/8ChwQ6wHEJHLgNnACxML0WSCm9bM58oLZ/K3v0jc8or+ZptZc7zWLkrtxdKbT5/llX3tfDKDR+BGiyXpzweaI+63uNui3Swi20XkcRGpAhCRLOC7wJ3nO4CI3CYijSLS2N7eHmPoJp2ICPfduIL+oWH+clNillf0B4JUFOaxoMwGZcXq4jlFzMjLTtm6/k8bmhHgExk8AjdaLEl/pI/H6MLrM4BPVVcCLwE/drd/Edisqs2ch6o+oKq1qlpbWZnZ19BmMl/FDL78/hp+sfMYL+w6Fvf9+wMhVi8sQ8TO+GKVk53FmhRdLL359Fkea2zmfRfNYl6pfdCH5cTQpgWI/JhcALxj0hRVPRVx9wfA37g/vwu4SkS+CBQCeSLSparndAYbA87yipu2tvHNTbt494UVFE6L5Vd0bMHufg6d7ObjtZm3aMZk1fnK+ceX3qLj7AAlBblehzOqM70D/P7AKV7Zd5JX9rVz+NRZRODWK31eh5ZUYvmLagBqRGQR0ArcAnwqsoGIzFXVo+7d9cAeAFX9dESbW4FaS/jmfHKzneUVb/7+7/j7LXv5y/XL47Lf8IpdNhJ3/Op85ajCG4HTXHPxbK/DedvQsLKjtYNX3mrnlX0naQoEGRxWCvKyedfimdz6bh9XL6lkcWWh16EmlTGTvqoOisjtwBYgG/iRqu4SkXuBRlXdBNwhIuuBQeA0cGsCYzZpbs3CMj57RTU//v1hblw9n1VVk7/axh8IkiVwaZUNyhqvVVWl5GYLDYeDnif9tlAPr+xr5+V9J3l1/0lCZ50lHVfML+G2qxdzVU0la6pLmZZj4zBGE9N3Z1XdDGyO2vaNiJ/vBu4eYx8PAQ+NO0KTkb72oYvYsus4dz2xnWe+9AfkZk9uHGFTIMTFc4opyItPuSiTTM/L5pL5JZ7MuHm2f5DXDp7i5becks2B9m4AZhdP4/1LZ3NVTQV/cGEFMwunTXlsqcr+AkxSKsrP5Vs3LOcL//4GD75yiP/x3gsmvK+hYWVrc4gbVs2LY4SZpc5XzkOvHqZ3YCiho5mHh5XdRzt5eV87r7x1ksYjpxkYUvJzs7h80Uw2rF3I1UsqqZlVaB3yE2RJ3yStDy2fw4eWz+Z7L73Fh1fMoXrmjAnt50B7F119g3Z9/iTU+cp54OWDbG/pePva/Xg53tn7dufrb/ed5JS7xsLSucX8yZWLuKqmklpfmU2dESeW9E1S+9b6S3j/P/yGe57aycN/snZCZ3dNR2ylrMm6rNr5wGw4fHrSSb93YIj6Q6d52e2A3XvcmXOpojCPq5dUOiWbmgpmFdk8OYlgSd8ktTkl+fzZuov4i6d38dTWVj62evyXXPoDIUoLcllUMbFvCsaZKuPCWYUTul5fVXnz2Ble2eck+dcPnaZ/cJi87CzqFpVx05qLuaqmkovnFJFlo2YTzpK+SXqfvryaJ/2tfPvZPbxnySzKZ+SN6/n+5iCrq0qtBjxJdb5ynt3extCwjjmlQfuZPl7df9Kpze87SfuZPgCWzC7kD6+o5qqaCi5fNJPpeVaymWqW9E3Sy3KXV/zIP/2W+57bw3c/cWnMz+3sHWDfiS4+stI6cSerzlfGxvoAe4+dYdm84nc81jc4xBuHg/zG7YDdfbQTgLKCXP6gxinZXFVTYesSJwFL+iYlXDynmC+8ZzH3/+oAN6+Zz7svrIjpeduaQ6haPT8e6txFVRqPnGbp3CL2n+jiZbcD9rWDp+gdGCYnS7isuow7P3QRV9dUsnxesZVskowlfZMyvnRNDc9tP8rXf76D579ydUxXc/gDIUTg0jgM8Mp0C8qmM6c4nwdePsj3f32Aox3ObKiLK2dwS91Cp2SzeGbcps4wiWH/OyZl5Odmc9/HVvDpB1/n//7nPu780MVjPqcpEKRmViHF+ck7Z0yqEBHWXTKHn/tbufLCmdxRU8kfXFhBVXmB16GZcbCkb1LKlRdWcPOaBfzrbw7y0UvncfGc4lHbqir+QIh1y+dMYYTp7S/XL+ebH11mneIpzNbINSnnz69fSvH0XO5+csd5l1c8dLKbjp4Bq+fHmSX81GZJ36Sc8hl5/MVHluIPhPjJ60dGbdcUcFfKqraRuMaEWdI3KenGVfO5qqaCv3l+L8c6Rl5e0R8IUjQthwttal1j3mZJ36QkEeH/3HgJA0PDfHPTzhHb+AMhLq0qtUsGjYlgSd+krOqZM/jK+5ewZddxtkQtr3i2f5A3j3Wyxur5xryDJX2T0j531SIunlPEN5/exZnegbe3b2vuYFhhtc2sacw7WNI3KS03O4u/vnklx8/08vdb9r693d/szKwZj1W3jEknlvRNyltVVcofvcvHw68doSngJHt/IMTiihmUjXNyNmPSnSV9kxa+9qGLmFOcz9ef3MHA0DD+QJBVVs835hyW9E1aKJyWw703XMKbx87wjad3cbKr3+r5xowgpqQvIutEZK+I7BeRu0Z4/FYRaReRre7tc+72VSLyexHZJSLbReST8X4BxoR9YNlsrrtkDhvrAwCstnq+MecYc+4dEckG7gc+ALQADSKySVV3RzX9qareHrXtLPBZVd0nIvOAN0Rki6qG4hG8MdH+cv1yfrvvJIPDysVzirwOx5ikE8uEa2uB/ap6EEBEHgVuAKKT/jlU9a2In9tE5ARQCVjSNwkxuzif792yiraOXnKyrXppTLRYkv58oDnifgtw+QjtbhaRq4G3gP+lqpHPQUTWAnnAgegnishtwG0ACxcujC1yY0Zx7dLZXodgTNKK5VRopDHs0VMbPgP4VHUl8BLw43fsQGQu8O/AH6vq8Dk7U31AVWtVtbaysjK2yI0xxoxbLEm/BaiKuL8AaItsoKqnVLXPvfsD4LLwYyJSDDwH3KOqr00uXGOMMZMRS9JvAGpEZJGI5AG3AJsiG7hn8mHrgT3u9jzg58DDqvqz+IRsjDFmosas6avqoIjcDmwBsoEfqeouEbkXaFTVTcAdIrIeGAROA7e6T/8EcDUwU0TC225V1a3xfRnGGGNiIaqjrzzkhdraWm1sbPQ6DGOMSSki8oaq1o7Vzq5pM8aYDGJJ3xhjMoglfWOMySBJV9MXkXZg9NWux1YBnIxTOImWSrFCasWbSrFCasWbSrFCasU7mVirVXXMgU5Jl/QnS0QaY+nMSAapFCukVrypFCukVrypFCukVrxTEauVd4wxJoNY0jfGmAySjkn/Aa8DGIdUihVSK95UihVSK95UihVSK96Ex5p2NX1jjDGjS8czfWOMMaOwpG+MMRkkLZK+iFSJyK9EZI+7Hu+XvY7pfEQkX0TqRWSbG++3vI5pLCKSLSJ+EXnW61jGIiKHRWSHu15zUk/kJCKlIvK4iLzp/v6+y+uYRiMiF0Wsg71VRDpF5CtexzUaEflf7t/XThHZKCL5Xsc0GhH5shvnrkS/p2lR03endp6rqk0iUgS8Adw4wjq+SUFEBJihql0ikgv8FvhyMq83ICJfBWqBYlX9iNfxnI+IHAZqVTXpB+SIyI+BV1T1QXcq8oJUWEPaXTu7FbhcVSczmDIhRGQ+zt/VMlXtEZHHgM2q+pC3kZ1LRC4BHsVZmrYfeB74H6q6LxHHS4szfVU9qqpN7s9ncObzn+9tVKNTR5d7N9e9Je2nr4gsAK4HHvQ6lnTiLjB0NfBDAFXtT4WE77oWOJCMCT9CDjBdRHKAAqIWf0oiS4HXVPWsqg4CvwE+lqiDpUXSjyQiPmA18Lq3kZyfWy7ZCpwAXlTVZI73e8CfAucsdZmkFHhBRN5w119OVouBduDf3NLZgyIyw+ugYnQLsNHrIEajqq3A3wMB4CjQoaoveBvVqHYCV4vITBEpAD7MO1crjKu0SvoiUgg8AXxFVTu9jud8VHVIVVfhLD+51v2Kl3RE5CPACVV9w+tYxuFKVV0DXAf8TxG52uuARpEDrAG+r6qrgW7gLm9DGptbhloPJO1qeCJSBtwALALmATNE5DPeRjUyVd0D/A3wIk5pZxvOglQJkTZJ362NPwH8RFWf9DqeWLlf538NrPM4lNFcCax36+SPAteIyH94G9L5qWqb++8JnOU613ob0ahagJaIb3mP43wIJLvrgCZVPe51IOfxfuCQqrar6gDwJPBuj2Malar+UFXXqOrVOKsPJqSeD2mS9N2O0R8Ce1T1H7yOZywiUikipe7P03F+Qd/0NqqRqerdqrpAVX04X+n/U1WT8owJQERmuJ35uKWSD+J8fU46qnoMaBaRi9xN1wJJefFBlA0kcWnHFQCuEJECNz9ci7t2dzISkVnuvwuBm0jg+zvmGrkp4krgD4Edbp0c4OuqutnDmM5nLvBj9wqILOAxVU36SyFTxGzg587fOTnAI6r6vLchndeXgJ+4JZODwB97HM95uTXnDwBf8DqW81HV10XkcaAJp1TiJ7mnY3hCRGYCA8D/VNVgog6UFpdsGmOMiU1alHeMMcbExpK+McZkEEv6xhiTQSzpG2NMBrGkb4wxGcSSvjHGZBBL+sYYk0H+H3ZhIMrXOMrtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(\"Standard Deviation:\", np.std(score_list))\n",
    "print(indices)\n",
    "plt.figure()\n",
    "plt.plot(indices, score_list)\n",
    "plt.title('Performance across Folds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which features are important during the time when the datasets are accurate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "0.5689477512425748\n",
      "0.5690025375756393\n",
      "0.5206339712918661\n",
      "0.6595706618962434\n",
      "0.5302161025052592\n",
      "0.5385946074922453\n",
      "0.61524064171123\n"
     ]
    }
   ],
   "source": [
    "indices = [x for x in range(len(score_list)) if score_list[x] > 0.5]\n",
    "importance = np.array([])\n",
    "count = 0\n",
    "\n",
    "for idx in range(0, len(trainDataPartition)): # for several months of 2012, May - end of November, 1180, 1281\n",
    "    # run random forest\n",
    "    try:\n",
    "\n",
    "        X_train, y_train = trainDataPartition[idx][:,1:data_size[1]+1], trainDataPartition[idx][:,0]\n",
    "        y_train = y_train.astype('int')\n",
    "\n",
    "        X_test, y_test = valDataPartition[idx][:,1:data_size[1]+1], valDataPartition[idx][:,0]\n",
    "        y_test = y_test.astype('int')\n",
    "    \n",
    "        # Fit the RF model\n",
    "        clf = RandomForestClassifier(n_estimators=50, max_depth=1500, random_state=0) # previously 7\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # print predicitions\n",
    "        pred = clf.predict(X_test)\n",
    "    \n",
    "        if not importance.any():\n",
    "            print('hi')\n",
    "            importance = clf.feature_importances_\n",
    "        else:\n",
    "            importance = importance + clf.feature_importances_\n",
    "\n",
    "        count+=1\n",
    "    except:\n",
    "        continue #np.nan\n",
    "\n",
    "    try:\n",
    "        temp_score = sklearn.metrics.roc_auc_score(y_test, pred)\n",
    "        score_list.append(temp_score)\n",
    "        indices.append(idx)\n",
    "        print(temp_score)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "importance = importance / count\n",
    "feature_indices = np.argsort(importance)[::-1]\n",
    "\n",
    "# Plot the feature importances of the forest (ERROR HIDDEN, TOO BIG)\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_train.shape[1]), importance[feature_indices],\n",
    "       color=\"r\", align=\"center\") # X_train.shape[1]\n",
    "plt.xticks(range(X_train.shape[1]), feature_indices) #X_train.shape[1]\n",
    "plt.xlim([-1, 100]) #X_train.shape[1]]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Features listed by Importance\n",
      "main98 0.032547854946930535\n",
      "main96 0.019119054039992946\n",
      "main93 0.013592680305261515\n",
      "main95 0.013252520558246593\n",
      "main68 0.012564630031937585\n",
      "main83 0.012549761531015408\n",
      "gfp3 0.011313464528692802\n",
      "vix18 0.011178232378648388\n",
      "main90 0.011044137657546468\n",
      "main91 0.011030078491105672\n",
      "invest11 0.010499385250520804\n",
      "invest13 0.010395887192760498\n",
      "main88 0.010237168873984197\n",
      "gfp5 0.009868793397333559\n",
      "main85 0.009856218497265274\n",
      "invest1 0.009854977688186847\n",
      "gfp4 0.009311184441166421\n",
      "main61 0.009309827054509556\n",
      "main66 0.009286152337792203\n",
      "main92 0.009105296391363798\n",
      "main63 0.009086971141057247\n",
      "main71 0.008666412517134039\n",
      "main97 0.00852553765108417\n",
      "main40 0.008485727382150096\n",
      "main78 0.00836297684032407\n",
      "main11 0.008285945025411847\n",
      "invest5 0.008261894756389662\n",
      "gfp1 0.00823294558056122\n",
      "vix6 0.008207850341607549\n",
      "main4 0.008203632402517804\n",
      "gfp2 0.008180307561250324\n",
      "main32 0.008139721072370579\n",
      "main30 0.008124633063718224\n",
      "main7 0.008028928172253019\n",
      "main6 0.008009736203567505\n",
      "main1 0.00798370285663723\n",
      "main17 0.007973482723900041\n",
      "main12 0.00796958700645326\n",
      "main55 0.007956011624935708\n",
      "main80 0.00791710805183437\n",
      "main81 0.007877194647538919\n",
      "main94 0.007750986739476791\n",
      "main82 0.007708482979833933\n",
      "vix2 0.0076837051863592925\n",
      "vix5 0.007608493036134377\n",
      "invest3 0.007582378395105493\n",
      "main47 0.007556604382244111\n",
      "main57 0.007532251624335044\n",
      "main45 0.007446524193879526\n",
      "main50 0.00742545881587984\n",
      "cpi0 0.007394118772627342\n",
      "main13 0.007393020192686907\n",
      "main25 0.007381694851076182\n",
      "main52 0.007335106800527381\n",
      "vix16 0.007328960716227269\n",
      "main33 0.007272177372409747\n",
      "main65 0.0072352631179911295\n",
      "invest7 0.007175586268374066\n",
      "main2 0.007136379070865506\n",
      "main35 0.007130600016146858\n",
      "main86 0.007012499321589978\n",
      "vix10 0.0069542278034541785\n",
      "gfp0 0.006921201043107672\n",
      "Label_m30 0.006853645469676123\n",
      "main87 0.006829382923044304\n",
      "vix1 0.006806178702650124\n",
      "main27 0.006676099700953439\n",
      "main75 0.006618294531966448\n",
      "main53 0.006593736394754872\n",
      "rdi0 0.0065804356755035905\n",
      "main76 0.006575980871770339\n",
      "main10 0.006564277479633147\n",
      "cci0 0.006464780665246776\n",
      "vix14 0.0064384361651408095\n",
      "main14 0.0063846857356986446\n",
      "main22 0.006373749589562415\n",
      "invest0 0.006330720625321446\n",
      "main24 0.006314228656573705\n",
      "vix7 0.006281051470655466\n",
      "main21 0.006250310656046446\n",
      "main15 0.00621258017264416\n",
      "main59 0.0062061402634663336\n",
      "main73 0.006159105776294366\n",
      "invest8 0.006144367432662609\n",
      "main44 0.006108467368054764\n",
      "main20 0.006094728033590547\n",
      "main70 0.00605188348802249\n",
      "main39 0.006034805597014006\n",
      "main56 0.006032453293889199\n",
      "main18 0.006023736928454807\n",
      "main62 0.0060219349086742474\n",
      "main29 0.005801219949831448\n",
      "main5 0.005793310481219657\n",
      "main60 0.005787057151194564\n",
      "main54 0.005782639228298271\n",
      "main8 0.005756828736472915\n",
      "main9 0.005682671501868362\n",
      "vix11 0.005609472173394897\n",
      "main19 0.0055629639948078695\n",
      "main34 0.005555022152899881\n",
      "main67 0.005522189062164377\n",
      "main0 0.005485424411950019\n",
      "vix8 0.0054187375535977534\n",
      "main43 0.0053507661393395614\n",
      "main37 0.0053499339708312985\n",
      "vix0 0.005321165606801909\n",
      "main58 0.005277252546147722\n",
      "invest12 0.005276014412375546\n",
      "main72 0.005083414610761676\n",
      "vix17 0.0050333256480712555\n",
      "vix3 0.004983211191093995\n",
      "main3 0.004942302909810584\n",
      "main23 0.00494034741809124\n",
      "main99 0.004927401845700019\n",
      "vix15 0.004924626137323166\n",
      "main28 0.004728852274618484\n",
      "vix12 0.004619405185214027\n",
      "invest9 0.004602591391831909\n",
      "main77 0.0045643341369932325\n",
      "main49 0.0045496011806825404\n",
      "main89 0.004540059114508265\n",
      "vix9 0.0044463862554293245\n",
      "main46 0.004436810437708233\n",
      "main41 0.004397578099735708\n",
      "main74 0.00438454690342063\n",
      "invest15 0.0043295403122529325\n",
      "main26 0.004296577766208371\n",
      "main36 0.004205362007900284\n",
      "main38 0.00419339316395599\n",
      "main16 0.004144221273934489\n",
      "vix13 0.004090519950591012\n",
      "main84 0.0038832245512294104\n",
      "main64 0.0038057295161597854\n",
      "invest4 0.0037654076140019115\n",
      "main79 0.0037457153979566335\n",
      "vix4 0.00370402934982028\n",
      "main51 0.0035982156371278684\n",
      "main42 0.0033247552977935797\n",
      "main48 0.0032797251292371083\n",
      "main31 0.003193558448736015\n",
      "vix19 0.002314295822271361\n",
      "invest10 0.002190070757687709\n",
      "main69 0.001792277819864822\n",
      "invest14 0.0012080463054036249\n",
      "invest2 0.0010937724795049531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['main98',\n",
       " 'main96',\n",
       " 'main93',\n",
       " 'main95',\n",
       " 'main68',\n",
       " 'main83',\n",
       " 'gfp3',\n",
       " 'vix18',\n",
       " 'main90',\n",
       " 'main91',\n",
       " 'invest11',\n",
       " 'invest13',\n",
       " 'main88',\n",
       " 'gfp5',\n",
       " 'main85',\n",
       " 'invest1',\n",
       " 'gfp4',\n",
       " 'main61',\n",
       " 'main66',\n",
       " 'main92',\n",
       " 'main63',\n",
       " 'main71',\n",
       " 'main97',\n",
       " 'main40',\n",
       " 'main78',\n",
       " 'main11']"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'Combined_Sets_from_Revised.csv'\n",
    "df = pd.read_csv(name)\n",
    "df = df.drop(['Date'], axis = 1)\n",
    "\n",
    "# Show all the list of features and their respective importances\n",
    "\n",
    "#listoffeatures = list(train_pd)[1:]\n",
    "listoffeatures = list(df)\n",
    "shortlist = []\n",
    "\n",
    "print('Top Features listed by Importance')\n",
    "\n",
    "for i in range(len(feature_indices)-1):\n",
    "    idx = feature_indices[i]\n",
    "    print(listoffeatures[idx], importance[idx])\n",
    "    if i <= 25:\n",
    "        shortlist.append(listoffeatures[idx])\n",
    "shortlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xdaad0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a graph of fetaure importances\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot the feature importances of the forest (ERROR HIDDEN, TOO BIG)\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(100), importances[indices[0:100]],\n",
    "       color=\"r\", yerr=std[indices[0:100]], align=\"center\") # X_train.shape[1]\n",
    "plt.xticks(range(100), indices[0:100]) #X_train.shape[1]\n",
    "plt.xlim([-1, 100]) #X_train.shape[1]]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217\n",
      "218\n"
     ]
    }
   ],
   "source": [
    "print(len(clf.feature_importances_))\n",
    "print(len(list(train_pd)[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '../Data/Combined_Sets_from_Revised_3.csv'\n",
    "df = pd.read_csv(name)\n",
    "df = df.drop(['Date'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Features listed by Importance\n",
      "main56 0.04246516422174906\n",
      "main66 0.038890888318713866\n",
      "main81 0.03352209371117746\n",
      "main31 0.03316343824589267\n",
      "main71 0.029538007701172927\n",
      "main61 0.027638836562973338\n",
      "main96 0.026174527778898057\n",
      "main41 0.024717976602409993\n",
      "main33 0.022179143584261544\n",
      "main86 0.02192126337972254\n",
      "google_hits40 0.018536965272188975\n",
      "main26 0.018094151121507362\n",
      "main51 0.017907962611983155\n",
      "main53 0.01662669776174378\n",
      "main46 0.015511696529390826\n",
      "main6 0.015034778619333933\n",
      "invest5 0.014968034625538185\n",
      "google_hits53 0.014830217360014095\n",
      "main28 0.01446104855036105\n",
      "main36 0.013716114196511751\n",
      "main48 0.01369902198463893\n",
      "main93 0.012202968050008907\n",
      "main76 0.011880261213021217\n",
      "main73 0.010328238974565888\n",
      "main16 0.010052793341765304\n",
      "main19 0.009764406723616902\n",
      "invest3 0.009464639641187931\n",
      "main11 0.00859702479350911\n",
      "main88 0.008526362192154482\n",
      "main39 0.008260623177502284\n",
      "main24 0.008066004010519183\n",
      "main82 0.007844714248476117\n",
      "gfp8 0.007815534136366665\n",
      "main87 0.007041696130779015\n",
      "google_hits65 0.006967652742487809\n",
      "google_hits20 0.006902302692713001\n",
      "main2 0.006800220422439751\n",
      "main38 0.006738662783590923\n",
      "invest1 0.006597739086418496\n",
      "main52 0.006339177248413949\n",
      "invest7 0.006307781484506752\n",
      "main58 0.006137931478150553\n",
      "main54 0.0060628722019664535\n",
      "main30 0.005982808348576838\n",
      "main77 0.005828524516943372\n",
      "main98 0.005702826660309271\n",
      "google_hits1 0.005663721061424887\n",
      "google_hits12 0.005563208157980285\n",
      "gfp7 0.00548422924096102\n",
      "main59 0.005466601014086998\n",
      "main42 0.005196900152777917\n",
      "google_hits3 0.005152276044183217\n",
      "main44 0.005150007039899013\n",
      "main72 0.005074676182816275\n",
      "main63 0.005029383262084828\n",
      "main10 0.004945854569618633\n",
      "vix18 0.004797443339778451\n",
      "main18 0.004714812437907577\n",
      "Label4 0.00470369689279444\n",
      "main1 0.004644546373997585\n",
      "main99 0.004527986106514567\n",
      "main95 0.0043830395208108225\n",
      "google_hits0 0.004073396961495436\n",
      "vix9 0.004059169379505675\n",
      "main67 0.004032692683430712\n",
      "main91 0.004028929423588098\n",
      "vix1 0.00402515245626404\n",
      "main92 0.0040232218743024226\n",
      "main43 0.00401571890577654\n",
      "vix0 0.003939623780565347\n",
      "main34 0.0038822913547981487\n",
      "main68 0.003802651361683539\n",
      "main20 0.0037648070900256785\n",
      "main7 0.003759244533217392\n",
      "main29 0.0036656377887703774\n",
      "main5 0.0036636422302611816\n",
      "main64 0.003641033385107274\n",
      "main94 0.0035885108172296265\n",
      "main49 0.0035316821411182776\n",
      "main27 0.0035173712999337016\n",
      "google_hits35 0.003471247199402539\n",
      "gfp5 0.003456021943672014\n",
      "main47 0.003445145251149178\n",
      "google_hits43 0.003403563964839724\n",
      "vix16 0.0033554692108385326\n",
      "main22 0.003316328415358485\n",
      "google_hits49 0.0032464408121827258\n",
      "main15 0.0032103601363147136\n",
      "main37 0.0030942871372572696\n",
      "main17 0.003086096631638091\n",
      "main3 0.003067301295346146\n",
      "main85 0.00303931838657808\n",
      "main35 0.0030255478676819436\n",
      "main74 0.002999369153542762\n",
      "main4 0.0028407014921798053\n",
      "vix13 0.0028321760797555963\n",
      "main32 0.0028196904109299386\n",
      "main84 0.0028106539461164332\n",
      "invest0 0.0027952976449841897\n",
      "vix8 0.002738965492595124\n",
      "main78 0.0027267734923204705\n",
      "main45 0.0027262499405578417\n",
      "gfp0 0.002706027812218724\n",
      "main25 0.0026991280660191635\n",
      "vix2 0.002682960569055477\n",
      "main13 0.002655515444249283\n",
      "main23 0.0025986793925946404\n",
      "main9 0.002597453068736458\n",
      "vix10 0.0025622912016360355\n",
      "main14 0.002541476139927028\n",
      "gfp6 0.0024359672060920135\n",
      "google_hits36 0.0023466381249853956\n",
      "main69 0.0023417365217135583\n",
      "main75 0.002326799991991365\n",
      "google_hits41 0.0022938901601567137\n",
      "google_hits55 0.0022017612430552196\n",
      "google_hits46 0.002191289182091685\n",
      "main89 0.0021638608236840425\n",
      "main40 0.0021448310781357702\n",
      "google_hits14 0.002119607647941866\n",
      "main57 0.002102774380961217\n",
      "main12 0.002087364849299131\n",
      "main90 0.0020825736447060528\n",
      "vix12 0.002075792526924986\n",
      "vix17 0.002055264110711405\n",
      "google_hits8 0.0020351104948871015\n",
      "vix6 0.001980222013967345\n",
      "google_hits16 0.0018986514158922494\n",
      "gfp4 0.0018977718330979187\n",
      "main70 0.001873467870843899\n",
      "rdi0 0.0018079115529388839\n",
      "vix11 0.0017768750622082052\n",
      "google_hits26 0.0017639654445704383\n",
      "vix15 0.0017318116909896212\n",
      "main62 0.001726250660094476\n",
      "vix4 0.0016725469298374917\n",
      "invest8 0.0016504855635974276\n",
      "invest12 0.0015290493868347593\n",
      "main0 0.0015067575908122755\n",
      "google_hits28 0.0014288303853314957\n",
      "invest14 0.0013963755876898803\n",
      "cci0 0.0013763729265254946\n",
      "invest9 0.0013735351345612384\n",
      "vix3 0.001294758570116214\n",
      "vix7 0.001269725371730423\n",
      "main83 0.0011486006284853036\n",
      "main97 0.0011105719612005598\n",
      "google_hits15 0.0011046576325825779\n",
      "google_hits42 0.0011006502919880898\n",
      "main65 0.0010965310360449608\n",
      "main60 0.0010736784598493572\n",
      "vix5 0.0010628951798793704\n",
      "main8 0.001051479470812033\n",
      "gfp3 0.0009969655797554666\n",
      "gfp2 0.000965950447795599\n",
      "main80 0.0009654015186774373\n",
      "google_hits11 0.0009593027609585864\n",
      "main50 0.000943181082521658\n",
      "main55 0.0009354148792477555\n",
      "main21 0.0008936841327753135\n",
      "google_hits45 0.0008705605624963544\n",
      "google_hits7 0.0008471130356327895\n",
      "invest13 0.0008263382853872618\n",
      "google_hits37 0.0007610893041427097\n",
      "gfp1 0.0007076204665326055\n",
      "google_hits58 0.0006218023255813958\n",
      "vix14 0.000571428956363824\n",
      "google_hits51 0.0005701161770140432\n",
      "google_hits48 0.0005579085266585267\n",
      "google_hits5 0.0005564701414010599\n",
      "google_hits27 0.0005495328641138614\n",
      "invest15 0.00053950576768973\n",
      "invest4 0.0004541591839841816\n",
      "main79 0.00040620922140060904\n",
      "google_hits17 0.00036046511627906973\n",
      "invest11 0.00030692670936438847\n",
      "google_hits6 0.0002868710237131289\n",
      "gfp9 0.0002170868347338936\n",
      "invest6 0.00020172953417196837\n",
      "invest10 0.00016450471698113207\n",
      "google_hits10 0.00015071417394997493\n",
      "google_hits54 0.00010905253283302064\n",
      "google_hits25 9.277633765982651e-05\n",
      "invest2 1.9320647409485103e-05\n",
      "google_hits66 5.6140083917407375e-06\n",
      "google_hits24 0.0\n",
      "google_hits23 0.0\n",
      "google_hits64 0.0\n",
      "google_hits63 0.0\n",
      "google_hits62 0.0\n",
      "google_hits61 0.0\n",
      "google_hits60 0.0\n",
      "google_hits59 0.0\n",
      "vix19 0.0\n",
      "google_hits57 0.0\n",
      "cpi0 0.0\n",
      "google_hits56 0.0\n",
      "google_hits2 0.0\n",
      "google_hits52 0.0\n",
      "google_hits4 0.0\n",
      "google_hits50 0.0\n",
      "google_hits9 0.0\n",
      "google_hits47 0.0\n",
      "google_hits13 0.0\n",
      "google_hits44 0.0\n",
      "google_hits18 0.0\n",
      "google_hits39 0.0\n",
      "google_hits38 0.0\n",
      "google_hits19 0.0\n",
      "google_hits22 0.0\n",
      "google_hits34 0.0\n",
      "google_hits33 0.0\n",
      "google_hits32 0.0\n",
      "google_hits31 0.0\n",
      "google_hits30 0.0\n",
      "google_hits29 0.0\n"
     ]
    }
   ],
   "source": [
    "# Show all the list of features and their respective importances\n",
    "\n",
    "#listoffeatures = list(train_pd)[1:]\n",
    "listoffeatures = list(df)\n",
    "shortlist = []\n",
    "\n",
    "print('Top Features listed by Importance')\n",
    "\n",
    "for i in range(len(indices)-1):\n",
    "    idx = indices[i]\n",
    "    print(listoffeatures[idx], importances[idx])\n",
    "#    if i <= 50:\n",
    "#        shortlist.append(listoffeatures[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "trainDataPartition, valDataPartition, group = time_cross(train, 0, 410, 50) #10 252 100 Draft_Google_shorter 0.8795"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(valDataPartition[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 173 306\n",
      "test: 40 49\n",
      "train: 219 388\n",
      "test: 12 49\n",
      "train: 178 314\n",
      "test: 23 49\n",
      "train: 205 363\n",
      "test: 22 49\n",
      "Averaged Score is: nan 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "splits = group\n",
    "score = []\n",
    "good_sets = []\n",
    "gammas = []\n",
    "\n",
    "data_size = trainDataPartition[0].shape\n",
    "\n",
    "for idx in range(len(trainDataPartition)-1):\n",
    "    try:\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, y_train = trainDataPartition[idx][:,1:data_size[1]+1], trainDataPartition[idx][:,0]\n",
    "        y_train = y_train.astype('int')\n",
    "        #print(X_train)\n",
    "        X_test, y_test = valDataPartition[idx][:,1:data_size[1]+1], valDataPartition[idx][:,0]\n",
    "        y_test = y_test.astype('int')\n",
    "        print('train:', sum(y_train), len(y_train))\n",
    "        print('test:', sum(y_test), len(y_test))\n",
    "        #print(1 / (X_train.shape[1] * X_train.var()))\n",
    "        # Fit the RF model\n",
    "\n",
    "        #gamma =  (1 / (X_train.shape[1] * X_train.var()))\n",
    "        clf = sklearn.svm.SVC(C=.7, kernel='rbf', gamma='scale') # previously 7\n",
    "        clf.fit(X_train, y_train)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # print predicitions\n",
    "    pred = clf.predict(X_test)\n",
    "    #print(pred)\n",
    "\n",
    "    # add up AUROCs\n",
    "            \n",
    "    try:\n",
    "        temp_score = sklearn.metrics.roc_auc_score(y_test, pred)\n",
    "        score.append(temp_score)\n",
    "        print(temp_score)\n",
    "        if temp_score > 0.7:\n",
    "            good_sets.append(idx)\n",
    "            gammas.append(1 / (X_train.shape[1] * X_train.var()))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# calculate average\n",
    "score = np.mean(score)\n",
    "print(\"Averaged Score is: %0.4f\" % score, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels2 = []\n",
    "for predlist in pred_labels:\n",
    "    for x in range(len(predlist)):\n",
    "        pred_labels2.append(predlist[x])\n",
    "pred_labels2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test just one giant set\n",
    "Why: Because if we can get more than 50%, we can get predictions for everything in the training set. Let's see if we can get some kind of \"universal\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7120, 219)"
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1 = \"Combined_Sets_from_Revised_3_Label1.csv\" #\"Draft_Google_Shorter.csv\" #Removed Missing\n",
    "\n",
    "train_pd = pd.read_csv(file1)\n",
    "train_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(936, 219)"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pd = train_pd[6184:]\n",
    "\n",
    "train = np.array(train_pd)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0, 786.5599980000001, 2.908, ..., 0.0, 0.0, 26.0],\n",
       "       [1.0, 789.5, 2.891, ..., 0.0, 0.0, 26.0],\n",
       "       [1.0, 783.119995, 2.944, ..., 0.0, 0.0, 26.0],\n",
       "       ...,\n",
       "       [1.0, 683.849976, 1.9669999999999999, ..., 1.0, 1.0, 36.0],\n",
       "       [1.0, 705.049988, 1.9580000000000002, ..., 1.0, 1.0, 36.0],\n",
       "       [1.0, 706.179993, 1.996, ..., 1.0, 1.0, 36.0]], dtype=object)"
      ]
     },
     "execution_count": 683,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valDataPartition[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
